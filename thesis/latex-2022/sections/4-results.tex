\section{Results and Discussion}
\label{sec:results}

\subsection{Lemmatization}
\label{subsec:lemmatization}

%% should I cite here too?
As outlined in \autoref{subsec:stanza-tagging} and \autoref{subsec:morfeusz-tagging}, the two lemmatization tools that were used in this thesis were Stanza and Morfeusz. The only evaluation measure that was obtained for lemmatization was accuracy, as lemmatization differs from other classification problems in terms of the sheer number of possible classes, and therefore other measures were considered superfluous. \autoref{table:lemmas} depicts the accuracy per model and per type of test data. What is immediately visible is Morfeusz's better performance on both modern and historical data. One potential explanation, that Stanza handles the input text worse than Morfeusz, has mostly been refuted by an inspection of the errors that both lemmatizers made and the fact that this tendency is consistent regardless of the type of data. Many of the mismatches between the gold standard lemma and the assigned lemma in the case of Stanza was due to Stanza returning all lemmas in lowercase by default, meaning that had capitalization been disregarded in this task, Stanza could have reached a higher accuracy score, both for modern and historical data.  \\

\renewcommand{\arraystretch}{1.25}
\begin{table}[h]
\begin{center}
\begin{tabular}{|cc|c|}
\hline \bf Model & \bf Data & \bf Accuracy \\ \hline
\multirow{2}{4em}{Stanza}
& PDB & 90.89\%  \\
& memoir & 83.49\%  \\
\multirow{2}{4em}{Morfeusz}
& PDB & 97.77\%  \\
& memoir & 91.01\% \\ 
\hline
\end{tabular}
\caption{\label{table:lemmas} Lemmatization accuracy per model and per test data type.}
\end{center}
\end{table}

Another noticeable difference is the significantly worse performance of the taggers on the historical data when compared to the modern data. The qualitative error analysis conducted on tokens that were mislabelled by both of the lemmatizers\footnote{This allowed for the elimination of errors caused by tagger-specific issues and made it possible to focus on instances where it was more likely that it was the token itself that was problematic; for instance, this eliminated nearly all of the instances where Stanza returned a lowercase lemma where it was not expected to do that.} revealed characteristics of the mislabelled tokens that could be identified by a human annotator. The error statistics can be seen in \autoref{table:lemmas-errors}, and explanations and examples of the specific error types can be found in \autoref{table:error-type-explanations} in \autoref{error-types}. Two major categories that can be distinguished from among the errors are the spelling-related ones and foreign vocabulary ones. The first category encompasses \textit{y}, \textit{e}, capitalization, abbreviation\footnote{This category does not include `spelling' as that error type is reserved for typos, while this overarching category is intended to gather tokens that are intentionally spelled in an unconventional way.}, while the latter - proper name, surname, name, foreign, archaic. Those two categories also appear to be rather text-specific, as the potentially unique vocabulary and a specific non-standard way of spelling are not features of the Polish language in general, but of the writing of this particular author. Naturally, there still are errors that stem from typos or ambiguities, but the bulk of the issues appear to be connected to the text's peculiarities. Simultaneously, the problematic tokens hint at there being a need for a more uniform way of determining what lemma to choose for verb-derived nouns and adjectives or words that have more than one acceptable spelling.

\renewcommand{\arraystretch}{1.25}
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf Error Type & \bf Raw Freq. & \bf Relative Freq. (\%) \\ \hline
\textit{y} & 35 & 24.14\%  \\
proper name & 30 & 20.69\%  \\
\textit{nie} & 19 & 13.10\%  \\
spelling & 12 & 8.28\% \\ 
surname & 12 & 8.28\% \\
capitalization & 8 & 5.52\% \\
abbreviation & 8 & 5.52\% \\
\textit{e} & 7 & 4.83\% \\
ambiguous & 3 & 2.07\% \\
name & 3 & 2.07\% \\
unidentified & 3 & 2.07\% \\
problematic & 2 & 1.38\% \\
foreign & 2 & 1.38\% \\
archaic & 1 & 0.69\% \\
\hline
\end{tabular}
\caption{\label{table:lemmas-errors} Types of errors and their raw and relative frequencies among the historical tokens that were mislabelled by both Stanza and Morfeusz.}
\end{center}
\end{table}

While a major drawback of this method of discovering a historical text's peculiarities is that there needs to be a gold standard to compare the performance of the lemmatizers to, it does reveal some interesting insights into the kinds of tokens that appear to be nonstandard for Polish and typical for a given text. What could be done in case of texts with no gold standard is simply reviewing the entire output - but that would, in the long run, be almost identical to manually lemmatizing it in the first place.

\subsection{UPOS-tagging}
\label{subsec:upos-tagging}

As detailed in \autoref{subsec:bert-tagging}, \autoref{subsec:marmot-tagging}, \autoref{subsec:stanza-tagging}, and \autoref{subsec:ud-tagging}, the four taggers that either utilized or could be trained to utilize the UD tagset were BERT, Marmot, Stanza, and the UD Cloud tagger. Unlike in the case of lemmatization, when it comes to tagging, there is a specific number of classes in question, which allowed for the extension of the evaluation metrics from just accuracy to accuracy, weighted precision, and weighted recall, which are presented in \autoref{table:upos}. Since the UD tagset is not large, precision and recall were also calculated for each class for a deeper insight into which classes are the most problematic \citep{ud-tagset}. These results can be found in \autoref{upos-class-measures}. Based on the general evaluation measures BERT performed best on both the modern and the historical dataset, while the UD Cloud tagger has the worst performance on both of the test sets. Although not by a lot, Stanza's neural pipeline outperforms Marmot. For all of the taggers the historical dataset achieves a consistently lower score than the modern one, indicating issues that are not specific to the taggers themselves. 

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|cc|cccc|}
\hline \bf Model & \bf Data & \bf Accuracy & \bf Precision & \bf Recall & \bf MCC \\ \hline
\multirow{2}{4em}{BERT}
& PDB & 99.20\% & 99.20\% & 99.20\% & 99.08\% \\
& memoir & 94.50\% & 94.72\% & 94.50\% & 93.77\%  \\
\multirow{2}{4em}{Marmot}
& PDB & 97.73\% & 97.75\% & 97.73\% & 97.38\% \\
& memoir & 90.61\% & 90.79\% & 90.61\% & 89.30\% \\
\multirow{2}{4em}{Stanza}
& PDB & 98.40\% & 98.41\% & 98.40\% & 98.16\% \\
& memoir & 93.31\% & 93.52\% & 93.31\% & 92.43\% \\
\multirow{2}{4em}{UD Cloud}
& PDB & 90.98\% & 91.17\% & 90.98\% & 89.59\% \\
& memoir & 83.41\% & 84.12\% & 83.41\% & 81.17\% \\ 
\hline
\end{tabular}
\caption{\label{table:upos} Evaluation measures (accuracy, precision (weighted), recall (weighted)), Matthew's Correlation Coefficient per model and per test data type. Although calculated, F1 is not given since it can be calculated from precision and recall. Per class precision and recall can be found in \autoref{upos-class-measures}}
\end{center}
\end{table}

Similarly as in the case of lemmatization, manual annotation of errors made by the taggers has revealed certain recurring features, as outlined in \autoref{table:pos-errors}; this time, however, the token in question did not have to be misclassified by all of the taggers, and instead only two of them had to have made a mistake while tagging a given token, as that was deemed more likely to still remove the tagger-specific issues while preserving more information on the possible problematic features of the data than if all of the taggers had to misclassify the token. A more detailed definition of the types of errors along with examples can be found in \autoref{table:error-type-upos-explanations} in \autoref{error-types}.

Similarly to the errors made during lemmatization, the errors can be divided into spelling-related and foreign vocabulary-related. However, another category appears to be taking shape here, namely that of form-related errors. These encompass a number of ambiguous, ending, problematic, and impersonal type errors, with the category of "ambiguous" being the most numerous error category in general. In that case the word itself appears to be classified according to the wrong meaning or just by the ending, which can itself be ambiguous and lead to different interpretations. Clearly, the word form itself (or the inflectional ending itself) should not be the only factor determining a word's class. In addition, some errors stem from the UD tagging rules which are complicated e.g. when it comes to the VERB vs. AUX distinction for the verb \textit{byÄ‡} in Polish. Although there is an overlap in terms of what features can lead to a token being misclassified, they seem to be of different importance when it comes to assigning UPOS tags compared to lemmatization.

UPOS tagging does share the same issue as lemmatization when it comes to the need for some gold standard to compare the tagging to. While it does hint at certain text-specific issues, such as nonstandard spelling or unusual vocabulary, the bulk of the errors appears to stem from the ambiguity of some tokens.

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf Error Type & \bf Raw Freq. & \bf Relative Freq. (\%) \\ \hline
ambiguous & 208 & 21.80\% \\
capitalization & 199 & 20.86\% \\
\textit{y} & 109 & 11.43\%  \\
unidentified & 62 & 6.50\% \\
archaic & 59 & 6.19\% \\
UD & 58 & 6.08\& \\
surname & 41 & 4.30\% \\
\textit{e} & 41 & 4.30\% \\
\textit{nie} & 28 & 2.94\% \\
ending & 24 & 2.56\% \\
spelling & 23 & 2.41\% \\
proper name & 21 & 2.20\% \\
problematic & 20 & 2.10\% \\
digits & 17 & 1.78\% \\
foreign & 13 & 1.36\% \\
uncommon & 12 & 1.26\% \\
abbreviation & 11 & 1.15\% \\
impersonal & 4 & 0.42\% \\
name & 2 & 0.21\% \\
currency & 1 & 0.11\% \\
special & 1 & 0.11\% \\
\hline
\end{tabular}
\caption{\label{table:pos-errors} Types of errors and their raw and relative frequencies among the historical tokens that were mislabelled by at least two of four UPOS taggers (BERT, Marmot, Stanza, UD Cloud).}
\end{center}
\end{table}

\subsection{XPOS-tagging}
\label{subsec:xpos-tagging}

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|cc|cccc|}
\hline \bf Model & \bf Data & \bf Accuracy & \bf Precision & \bf Recall & \bf MCC \\ \hline
\multirow{2}{4em}{BERT}
& PDB & 95.65\% & 95.13\% & 95.65\% & 95.47\% \\
& memoir & 89.39\% & 89.75\% & 89.39\% & 89.05\%  \\
\multirow{2}{4em}{Marmot}
& PDB & 89.27\% & 88.95\% & 89.27\% & 88.83\% \\
& memoir & 80.22\% & 81.34\% & 80.22\% & 79.60\% \\
\multirow{2}{4em}{Stanza}
& PDB & 94.29\% & 94.25\% & 94.29\% & 94.05\% \\
& memoir & 87.68\% & 88.44\% & 87.68\% & 87.28\% \\
\multirow{2}{4em}{Morfeusz}
& PDB & 94.43\% & 95.36\% & 94.43\% & 94.20\% \\
& memoir & 84.26\% & 86.83\% & 84.26\% & 83.76\% \\ 
\hline
\end{tabular}
\caption{\label{table:xpos} Evaluation measures (accuracy, precision (weighted), recall (weighted)), Matthew's Correlation Coefficient per model and per test data type. Although calculated, F1 is not given since it can be calculated from precision and recall.}
\end{center}
\end{table}

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf Error Type & \bf Raw Freq. & \bf Relative Freq. (\%) \\ \hline
ambiguous & 199 & 38.20\% \\
unidentified & 65 & 12.48\% \\
proper name & 52 & 9.98\% \\
\textit{y} & 39 & 7.49\% \\
digits & 25 & 4.80\% \\
problematic & 22 & 4.22\% \\
\textit{nie} & 20 & 3.84\% \\
spelling & 18 & 3.46\% \\
archaic & 17 & 3.26\% \\
foreign & 16 & 3.07\% \\
surname & 12 & 2.30\% \\
uncommon & 10 & 1.92\% \\
currency & 8 & 1.54\% \\
\textit{e} & 7 & 1.34\% \\
gender & 4 & 0.77\% \\
vocative & 3 & 0.58\% \\
abbreviation & 2 & 0.38\% \\
name & 2 & 0.38\% \\
\hline
\end{tabular}
\caption{\label{table:xpos-errors} Types of errors and their raw and relative frequencies among the historical tokens that were mislabelled by at least two of four XPOS taggers (BERT, Marmot, Stanza, Morfeusz).}
\end{center}
\end{table}

\subsection{n-gram statistics}
\label{subsec:ngram-stats-results}

\subsection{NKJP vocabulary comparison}
\label{subsec:nkjp-comparison-results}