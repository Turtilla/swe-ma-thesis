\section{Results and Discussion}
\label{sec:results}

% LEMMATIZATION
\subsection{Lemmatization}
\label{subsec:lemmatization}

%% should I cite here too?
As outlined in \autoref{subsec:stanza-tagging} and \autoref{subsec:morfeusz-tagging}, the two lemmatization tools that were used in this thesis were Stanza and Morfeusz. The only evaluation measure that was obtained for lemmatization was accuracy, as lemmatization differs from other classification problems in terms of the sheer number of possible classes, and therefore other measures were considered superfluous. \autoref{table:lemmas} depicts the accuracy per model and per type of test data. What is immediately visible is Morfeusz's better performance on both modern and historical data. One potential explanation, that Stanza handles the input text worse than Morfeusz, has mostly been refuted by an inspection of the errors that both lemmatizers made and the fact that this tendency is consistent regardless of the type of data. Many of the mismatches between the gold standard lemma and the assigned lemma in the case of Stanza was due to Stanza returning all lemmas in lowercase by default, meaning that had capitalization been disregarded in this task, Stanza could have reached a higher accuracy score, both for modern and historical data.  \\

\renewcommand{\arraystretch}{1.25}
\begin{table}[h]
\begin{center}
\begin{tabular}{|cc|c|}
\hline \bf Model & \bf Data & \bf Accuracy \\ \hline
\multirow{2}{4em}{Stanza}
& PDB & 90.89\%  \\
& memoir & 83.49\%  \\
\multirow{2}{4em}{Morfeusz}
& PDB & 97.77\%  \\
& memoir & 91.01\% \\ 
\hline
\end{tabular}
\caption{\label{table:lemmas} Lemmatization accuracy per model and per test data type.}
\end{center}
\end{table}

Another noticeable difference is the significantly worse performance of the taggers on the historical data when compared to the modern data. The qualitative error analysis conducted on tokens that were mislabelled by both of the lemmatizers\footnote{This allowed for the elimination of errors caused by tagger-specific issues and made it possible to focus on instances where it was more likely that it was the token itself that was problematic; for instance, this eliminated nearly all of the instances where Stanza returned a lowercase lemma where it was not expected to do that.} revealed characteristics of the mislabelled tokens that could be identified by a human annotator. The error statistics can be seen in \autoref{table:lemmas-errors}, and explanations and examples of the specific error types can be found in \autoref{table:error-type-explanations} in \autoref{error-types}. Two major categories that can be distinguished from among the errors are the spelling-related ones and foreign vocabulary ones. The first category encompasses \textit{y}, \textit{e}, \textit{nie}, capitalization, abbreviation\footnote{This category does not include `spelling' as that error type is reserved for typos, while this overarching category is intended to gather tokens that are intentionally spelled in an unconventional way.}, while the latter - proper name, surname, name, foreign, archaic. Those two categories also appear to be rather text-specific, as the potentially unique vocabulary and a specific non-standard way of spelling are not features of the Polish language in general, but of the writing of this particular author. Naturally, there still are errors that stem from typos or ambiguities, but the bulk of the issues appear to be connected to the text's peculiarities. Simultaneously, the problematic tokens hint at there being a need for a more uniform way of determining what lemma to choose for verb-derived nouns and adjectives or words that have more than one acceptable spelling.

\renewcommand{\arraystretch}{1.25}
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf Error Type & \bf Raw Freq. & \bf Relative Freq. (\%) \\ \hline
\textit{y} & 35 & 24.14\%  \\
proper name & 30 & 20.69\%  \\
\textit{nie} & 19 & 13.10\%  \\
spelling & 12 & 8.28\% \\ 
surname & 12 & 8.28\% \\
capitalization & 8 & 5.52\% \\
abbreviation & 8 & 5.52\% \\
\textit{e} & 7 & 4.83\% \\
ambiguous & 3 & 2.07\% \\
name & 3 & 2.07\% \\
unidentified & 3 & 2.07\% \\
problematic & 2 & 1.38\% \\
foreign & 2 & 1.38\% \\
archaic & 1 & 0.69\% \\
\hline
\end{tabular}
\caption{\label{table:lemmas-errors} Types of errors and their raw and relative frequencies among the historical tokens that were mislabelled by both Stanza and Morfeusz.}
\end{center}
\end{table}

While a major drawback of this method of discovering a historical text's peculiarities is that there needs to be a gold standard to compare the performance of the lemmatizers to, it does reveal some interesting insights into the kinds of tokens that appear to be nonstandard for Polish and typical for a given text. What could be done in case of texts with no gold standard is simply reviewing the entire output - but that would, in the long run, be almost identical to manually lemmatizing it in the first place.

% UPOS
\subsection{UPOS-tagging}
\label{subsec:upos-tagging}

As detailed in \autoref{subsec:bert-tagging}, \autoref{subsec:marmot-tagging}, \autoref{subsec:stanza-tagging}, and \autoref{subsec:ud-tagging}, the four taggers that either utilized or could be trained to utilize the UD tagset were BERT, Marmot, Stanza, and the UD Cloud tagger. Unlike in the case of lemmatization, when it comes to tagging, there is a specific number of classes in question, which allowed for the extension of the evaluation metrics from just accuracy to accuracy, weighted precision, weighted recall, and Matthew's Correlation Coefficient, which are presented in \autoref{table:upos}. Since the UD tagset is not large, precision and recall were also calculated for each class for a deeper insight into which classes are the most problematic \citep{ud-tagset}. These results can be found in \autoref{upos-class-measures}. Based on the general evaluation measures BERT performed best on both the modern and the historical dataset, while the UD Cloud tagger has the worst performance on both of the test sets. Although not by a lot, Stanza's neural pipeline outperforms Marmot. For all of the taggers the historical dataset achieves a consistently lower score than the modern one, indicating issues that are not specific to the taggers themselves. 

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|cc|cccc|}
\hline \bf Model & \bf Data & \bf Accuracy & \bf Precision & \bf Recall & \bf MCC \\ \hline
\multirow{2}{4em}{BERT}
& PDB & 99.20\% & 99.20\% & 99.20\% & 99.08\% \\
& memoir & 94.50\% & 94.72\% & 94.50\% & 93.77\%  \\
\multirow{2}{4em}{Marmot}
& PDB & 97.73\% & 97.75\% & 97.73\% & 97.38\% \\
& memoir & 90.61\% & 90.79\% & 90.61\% & 89.30\% \\
\multirow{2}{4em}{Stanza}
& PDB & 98.40\% & 98.41\% & 98.40\% & 98.16\% \\
& memoir & 93.31\% & 93.52\% & 93.31\% & 92.43\% \\
\multirow{2}{4em}{UD Cloud}
& PDB & 90.98\% & 91.17\% & 90.98\% & 89.59\% \\
& memoir & 83.41\% & 84.12\% & 83.41\% & 81.17\% \\ 
\hline
\end{tabular}
\caption{\label{table:upos} UPOS-tagging evaluation measures (accuracy, precision (weighted), recall (weighted)), Matthew's Correlation Coefficient per model and per test data type. Although calculated, F1 is not given since it can be calculated from precision and recall. Per class precision and recall can be found in \autoref{upos-class-measures}}
\end{center}
\end{table}

Similarly as in the case of lemmatization, manual annotation of errors made by the taggers has revealed certain recurring features, as outlined in \autoref{table:pos-errors}; this time, however, the token in question did not have to be misclassified by all of the taggers, and instead only two of them had to have made a mistake while tagging a given token, as that was deemed more likely to still remove the tagger-specific issues while preserving more information on the possible problematic features of the data than if all of the taggers had to misclassify the token. A more detailed definition of the types of errors along with examples can be found in \autoref{table:error-type-upos-explanations} in \autoref{error-types}.

Similarly to the errors made during lemmatization, the errors can be divided into spelling-related and foreign vocabulary-related. However, another category appears to be taking shape here, namely that of form-related errors. These encompass a number of ambiguous, ending, problematic, and impersonal type errors, with the category of "ambiguous" being the most numerous error category in general. In that case the word itself appears to be classified according to the wrong meaning or just by the ending, which can itself be ambiguous and lead to different interpretations. Clearly, the word form itself (or the inflectional ending itself) should not be the only factor determining a word's class; this may hint at an additional level of difficulty in terms of unusual word order if such information is utilized by the tagger. In addition, some errors stem from the UD tagging rules which are complicated e.g. when it comes to the VERB vs. AUX distinction for the verb \textit{byÄ‡} in Polish. What is worth noting is the high number of capitalization-related errors. It appears that capitalization is factored in as a feature when it comes to tagging words as PROPN - and, as a result, regular words written unexpectedly with a capital letter at the start are often misclassified as such. 

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf Error Type & \bf Raw Freq. & \bf Relative Freq. (\%) \\ \hline
ambiguous & 208 & 21.80\% \\
capitalization & 199 & 20.86\% \\
\textit{y} & 109 & 11.43\%  \\
unidentified & 62 & 6.50\% \\
archaic & 59 & 6.19\% \\
UD & 58 & 6.08\& \\
surname & 41 & 4.30\% \\
\textit{e} & 41 & 4.30\% \\
\textit{nie} & 28 & 2.94\% \\
ending & 24 & 2.56\% \\
spelling & 23 & 2.41\% \\
proper name & 21 & 2.20\% \\
problematic & 20 & 2.10\% \\
digits & 17 & 1.78\% \\
foreign & 13 & 1.36\% \\
uncommon & 12 & 1.26\% \\
abbreviation & 11 & 1.15\% \\
impersonal & 4 & 0.42\% \\
name & 2 & 0.21\% \\
currency & 1 & 0.11\% \\
special & 1 & 0.11\% \\
\hline
\end{tabular}
\caption{\label{table:pos-errors} Types of errors and their raw and relative frequencies among the historical tokens that were mislabelled by at least two of four UPOS taggers (BERT, Marmot, Stanza, UD Cloud).}
\end{center}
\end{table}

Class-specific measures yield a deeper insight into which classes in particular are more problematic. For BERT (as can be seen in \autoref{table:bert-pr}), all of the classes in the modern text, with the exception of INTJ and SYM perform very well. The two aforementioned classes could be problematic due to their scarcity. They are also not present at all in the historical data. As for the memoir, there is a noticeable drop in performance for ADV, AUX, PART, PROPN, SCONJ, and X - which are also less numerous than some of the other classes. Additionally, there is possibly some confusion between AUX and PART when it comes to the classification of the token \textit{to} `it' or `is'. Naturally, the numerous new proper nouns found in the historical text are also problematic, not to mention the issue of nonstandard capitalization. 

Marmot (\autoref{table:marmot-pr}) similarly struggles with SYM and INTJ in the modern data, but it also scores relatively lower on AUX, PART, PROPN, and X on the PDB test set. When it comes to the historical data, a noticeable drop in performance can be noticed for the ADJ category, alongside ADV, AUX, PART, PROPN, SCONJ, and X. Aside from the ADJ category (which is more complicated due to place name-derived adjectived being capitalized by the author and surnames being misclassified as adjectives), the same categories seem to be problematic for Marmot as for BERT, although in this case some of these issues are already noticeable on the modern test set.

Stanza's results (which can be found in \autoref{table:stanza-pr}) show high precision, but low recall on the problematic INTJ and SYM classes. Otherwise the tagger performs quite well across all of the categories in the modern data, with the possible exception of relatively low recall on PART. When it comes to the historical data, a noticeable drop in performance can be observed for ADJ, AUX, PROPN, SCONJ, and X, which again mostly overlaps with the categories that were problematic for the aforementioned taggers.

The UD Cloud tagger (\autoref{table:ud-pr}) shows a much more varied performance across the classes on both the modern and historical test data. As far as the PDB test set is concerned, it struggles with ADJ, ADV, AUX, NUM, PROPN, and X classes in particular. As for the memoir, a large drop in performance can be observed for most classes. Noticeably, the tagger performs well on ADP, CCONJ, and PUNCT, and has a high precision, but very low recall on DET. While the issues that this tagger has with the modern data do oartly overlap with the classes that were problematic for other taggers, this tendency is not that clear on the historical data due to the overall bad performance of the tagger. However, the issues visible on the PDB test set may hint to these classes being more problematic in both modern and historical data, and perhaps the errors made while classifying those are more visible due to the classes' lowqer frequencies.

Although there is an overlap in terms of what features can lead to a token being misclassified, they seem to be of different importance when it comes to assigning UPOS tags compared to lemmatization. UPOS tagging does share the same issue as lemmatization when it comes to the need for some gold standard to compare the tagging to. While it does hint at certain text-specific issues, such as nonstandard spelling or unusual vocabulary, the bulk of the errors appears to stem from the ambiguity of some tokens.

% XPOS
\subsection{XPOS-tagging}
\label{subsec:xpos-tagging}

In accordance with what was described in \autoref{subsec:bert-tagging}, \autoref{subsec:marmot-tagging}, \autoref{subsec:stanza-tagging}, and \autoref{subsec:morfeusz-tagging}, the four tools used for XPOS-tagging experiments were BERT, Marmot, Stanza's neural pipeline and the combination of the morphological analyzer Morfeusz and a morphosyntactic tagger Concraft-pl. All of these tools were previously used for either lemmatization or UPOS tagging. While the same general evaluation measures as in UPOS-tagging were employed in this task, a decision was made to leave out the tag-specific measures due to the sheer number of possible classes. The possible labels for the fine-tuned BERT model, extracted from the training and test set of PDB along with the historical data, consist of 898 different classes. 

The overall performance of the tools in the XPOS-tagging task was worse than in the UPOS-tagging one. This could be attributed to a larger number of classes that require finer distinctions to be made. Same as before, all of the tools perform noticeably worse on historical data compared to the modern data. The best performing tool on both test sets is BERT, and the worst - Marmot. What is worth pointing out in this case though is that Morfeusz and Concraft-pl's CRF architecture does outperform Stanza's neural pipeline slightly, but only on modern data.  

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|cc|cccc|}
\hline \bf Model & \bf Data & \bf Accuracy & \bf Precision & \bf Recall & \bf MCC \\ \hline
\multirow{2}{4em}{BERT}
& PDB & 95.65\% & 95.13\% & 95.65\% & 95.47\% \\
& memoir & 89.39\% & 89.75\% & 89.39\% & 89.05\%  \\
\multirow{2}{4em}{Marmot}
& PDB & 89.27\% & 88.95\% & 89.27\% & 88.83\% \\
& memoir & 80.22\% & 81.34\% & 80.22\% & 79.60\% \\
\multirow{2}{4em}{Stanza}
& PDB & 94.29\% & 94.25\% & 94.29\% & 94.05\% \\
& memoir & 87.68\% & 88.44\% & 87.68\% & 87.28\% \\
\multirow{2}{4em}{Morfeusz}
& PDB & 94.43\% & 95.36\% & 94.43\% & 94.20\% \\
& memoir & 84.26\% & 86.83\% & 84.26\% & 83.76\% \\ 
\hline
\end{tabular}
\caption{\label{table:xpos} XPOS-tagging evaluation measures (accuracy, precision (weighted), recall (weighted)), Matthew's Correlation Coefficient per model and per test data type. Although calculated, F1 is not given since it can be calculated from precision and recall.}
\end{center}
\end{table}

A manual error analysis and annotation has revealed a number of trends concerning the mistakes that the taggers make, as presented in \autoref{table:xpos-errors}. Definitions and examples of the errors can be found in \autoref{table:error-type-xpos-explanations} in \autoref{error-types}. A similar trend can be noticed in terms of the kinds of errors as in the UPOS-tagging task. The most numerous error type is the ambiguous errors, where one word form corresponds to multiple possible tags (e.g. in some declension paradigms certain cases have the same form). Once again, the prevalence of this errors hints either at the taggers not being able to properly utilize the contextual information that is necessary for the disambiguation of the class either due to their architecture or to the text's unusual word order; and again, the latter cannot be fully concluded simply from these results. 

One error category that is relatively prominent, namely digits, appears to stem from the tools utilizing different strategies when it comes to numbers written as digits. Some of them classify them as \textit{dig}, while others attempt to provide the tag as if the number were written out with letters. Another tagset-related issue is that of currency mistakes; the XPOS tagset generally divides masculine nouns into three subgenders: m1, which includes animate human nouns, m2, which includes animate non-human nouns, and m3, which includes inanimate nouns. These are supposed to reflect which form the determiner \textit{ktÃ³ry} `which' takes when referring to that noun \citep{ud-masculine-gender}. According to this, however, there are less obvious words that belong to the m2 category, such as currency names, which did seem to pose a problem for the taggers. In general, some mistakes were made when distinguishing between the m1 and m3 category for other tokens as well, but they were not classified separately.

Finally, although not numerous, two kinds of errors are worth mentioning in the context of the data, namely the gender and vocative ones. Although the gender category partly overlaps with the aforementioned masculine subgender disctinction, a number of the instances in this error category stem from the fact that not all personal pronouns in Polish reflect gender (the first and second person in both singular and plural do not have an overt marking of the gender on the pronoun itself). However, since some do, all of the personal pronouns are annotated for gender. This leads to a number of personal pronouns being misclassified as masculine, when they should, in fact, be feminine, as they are uttered by a female character and followed by feminine verb or adjective forms. As for the vocative category, when the word can be interpreted otherwise, the taggers tend to not utilize the tags for the vocative case. Both of these issues hint at the fact that the training data used for the taggers may be lacking in dialogues where both first and second person personal pronouns and nouns in the vocative case would be more present. Additionally, the tendency to select masculine over feminine for the pronouns may hint at a potential gender bias in the data.

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf Error Type & \bf Raw Freq. & \bf Relative Freq. (\%) \\ \hline
ambiguous & 199 & 38.20\% \\
unidentified & 65 & 12.48\% \\
proper name & 52 & 9.98\% \\
\textit{y} & 39 & 7.49\% \\
digits & 25 & 4.80\% \\
problematic & 22 & 4.22\% \\
\textit{nie} & 20 & 3.84\% \\
spelling & 18 & 3.46\% \\
archaic & 17 & 3.26\% \\
foreign & 16 & 3.07\% \\
surname & 12 & 2.30\% \\
uncommon & 10 & 1.92\% \\
currency & 8 & 1.54\% \\
\textit{e} & 7 & 1.34\% \\
gender & 4 & 0.77\% \\
vocative & 3 & 0.58\% \\
abbreviation & 2 & 0.38\% \\
name & 2 & 0.38\% \\
\hline
\end{tabular}
\caption{\label{table:xpos-errors} Types of errors and their raw and relative frequencies among the historical tokens that were mislabelled by at least two of four XPOS taggers (BERT, Marmot, Stanza, Morfeusz).}
\end{center}
\end{table}

The issues that plague the XPOS taggers resemble those characteristic of UPOS tagging, with some novel types of errors being more indicative of the issues with the tagset or the training data. Once again, while this method does hint at unusual vocabulary and spelling practices being present in the text, and allows for the noticing of patterns within those trends, it requires prior manual annotation, which makes it more difficult to utilize it at a larger scale.

% ngrams - tomorrow
\subsection{n-gram statistics}
\label{subsec:ngram-stats-results}

% nkjp
\subsection{NKJP vocabulary comparison}
\label{subsec:nkjp-comparison-results}

As discussed in \autoref{subsec:nkjp-vocab}, a comparison of the unique tokens and lemmas from both of the test sets against a subsection of the data from the National Corpus of Polish was conducted. As visualized in \autoref{table:nkjp-results}, noticeably more tokens and lemmas from the historical data were not found in the National Corpus of Polish when compared to the results from the PDB test set. The difference is statistically significant. 

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|cc|c|c|c|}
\hline \bf Data & \bf Data & \bf Total unique & \bf Not found & \bf \% \\ \hline
\multirow{2}{4em}{PDB}
& lemmas & 7583 & 44 & 0.58\%  \\
& tokens & 12601 & 56 & 0.44\%  \\
\multirow{2}{4em}{Historical}
& lemmas & 1213 & 86 & 7.09\%  \\
& tokens & 4302 & 346 & 8.04\% \\ 
\hline
\end{tabular}
\caption{\label{table:nkjp-results} Raw and \% numbers of tokens and lemmas unique to the modern or historical test sets when compared with a subset of the NKJP.}
\end{center}
\end{table}

An inspection of the items that were returned as having no matches in the National Corpus of Polish for the PDB test set (which can be found in the resources in \autoref{app-resources}) reveals that many of them overlapped across the lemma and token category (i.e. if a token was not found in the corpus, neither was its lemma, and vice versa). A number of the missing items include punctuation, which appears to not be included in the corpus searches, as well as different numbers and numerals, which, due to their practically infinite number, is understandable. Additionally, a number of place names, names, surnames, and brand names were not found in the NKJP. Finally new borrowings with nonstandard spelling reflecting the original pronunciation (the dimunitive \textit{lajwik} `live (stream)') as well as highly specialized vocabulary (\textit{trichlorobenzen} `trichlorobenzene') were not found in the National Corpus of Polish either, which is warranted as these are either new or very rarely used words.

A similar trend in terms of both the token and the lemma missing from the National Corpus of Polish can be noticed for the historical data, although here the comparison is more difficult, as the unique tokens were extracted from a larger text sample than the unique lemmas (which had to be manually annotated). Once again, some punctuation is listed as not found due to the search engine's limitations. However, almost no standard numbers or numerals are listed as not found, with the exception of \textit{cwansiger} `20 (coin/bill)', which appears to be a borrowing from German. Similarly as with the modern data, a large number of proper names and surnames was not found in the corpus. A large part of the vocabulary with no hits in the NKJP consists of either words spelled in a nonstandard fashion (with the spelling of \textit{nie} together with the verb it modifies and the use of the grapheme \textit{y} for /j/ being the most prominent; there is one interesting instance of the sound \textipa{/\textrtailz/} being spelled as \textit{rÅ¼}, while the two accepted modern spellings are \textit{rz} and \textit{Å¼}\footnote{These two spellings reflect a past phonemic merger.}), or of words that appear to be out of use or highly specific to the sociohistorical context of the text (such as \textit{mandatariat} `the position of being a potentiary' or \textit{mortyfikowaÄ‡} `to self-flagellate'), with a potential overlap between the two categories. What is also noticeable is that some of the words reappear with multiple variations of spelling (e.g. \textit{jurysdykcya}, \textit{juryzdyksya} `jurisdiction' or \textit{mandataryusz}, \textit{mandatyrusz} `potentiary') indicating a certain degree of inconsistency when it comes to spelling.

A simple comparison of the vocabulary of a given text or dataset to that of a large corpus of a given language appears to yield informative results as far as those texts' divergence from the standard is concerned. With the exclusion of terms that are naturally unlikely to appear in the corpus (surnames, proper names), there still remain tokens and lemmas that were not identified in the corpus due to their spelling or rarity, and some of the same conclusions as to the nature of these differences can be drawn here as from the experiments \autoref{subsec:lemmatization}, \autoref{subsec:upos-tagging}, and \autoref{subsec:xpos-tagging}. While this method does not return as many items that were not found (or, in case of the taggers, errors), it only requires the text to be lemmatized, not annotated for the part of speech - and even simply searching for the tokens, and not their lemmas, yields interesting results that are not extremely different from those for lemmas. Overall, this kind of a comparison appears to be quite rewarding for the amount of preprocessing or annotation required.