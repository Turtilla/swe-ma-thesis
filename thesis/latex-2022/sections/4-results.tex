\section{Results and Discussion}
\label{sec:results}

% LEMMATIZATION
\subsection{Lemmatization}
\label{subsec:lemmatization}

%% should I cite here too?
As outlined in \autoref{subsec:stanza-tagging} and \autoref{subsec:morfeusz-tagging}, the two lemmatization tools that were used in this thesis were Stanza and Morfeusz. The only evaluation measure that was obtained for lemmatization was accuracy, as lemmatization differs from other classification problems in terms of the sheer number of possible classes, and therefore other measures were considered superfluous. \autoref{table:lemmas} depicts the accuracy per model and type of test data. What is immediately visible is Morfeusz's better performance on both modern and historical data. One potential explanation, that Stanza handles the input text worse than Morfeusz, has mostly been refuted by an inspection of the errors that both lemmatizers made and the fact that this tendency is consistent regardless of the type of data. Many of the mismatches between the gold standard lemma and the assigned lemma in the case of Stanza were due to Stanza returning all lemmas in lowercase by default, meaning that had capitalization been disregarded in this task, Stanza could have reached a higher accuracy score, both for modern and historical data.  \\

\renewcommand{\arraystretch}{1.25}
\begin{table}[h]
\begin{center}
\begin{tabular}{|cc|c|}
\hline \bf Model & \bf Data & \bf Accuracy \\ \hline
\multirow{2}{4em}{Stanza}
& PDB & 90.89\%  \\
& memoir & 83.49\%  \\
\multirow{2}{4em}{Morfeusz}
& PDB & 97.77\%  \\
& memoir & 91.01\% \\ 
\hline
\end{tabular}
\caption{\label{table:lemmas} Lemmatization accuracy per model and per test data type.}
\end{center}
\end{table}

Another noticeable difference is the significantly worse performance of the taggers on the historical data when compared to the modern data. The qualitative error analysis conducted on tokens that were mislabelled by both of the lemmatizers\footnote{This allowed for the elimination of errors caused by tagger-specific issues and made it possible to focus on instances where it was more likely that it was the token itself that was problematic; for instance, this eliminated nearly all of the instances where Stanza returned a lowercase lemma where it was not expected to do that.} revealed characteristics of the mislabelled tokens that could be identified by a human annotator. The error statistics can be seen in \autoref{table:lemmas-errors}, and explanations and examples of the specific error types can be found in \autoref{table:error-type-explanations} in \autoref{error-types}. Two major categories that can be distinguished from among the errors are spelling-related ones and foreign vocabulary ones. The first category encompasses \textit{y}, \textit{e}, \textit{nie}, capitalization, abbreviation\footnote{This category does not include `spelling' as that error type is reserved for typos, while this overarching category is intended to gather tokens that are intentionally spelled in an unconventional way.}, while the latter - proper name, surname, name, foreign, archaic. Those two categories also appear to be rather text-specific, as the potentially unique vocabulary and a specific non-standard way of spelling are not features of the Polish language in general, but of the writing of this particular author. Naturally, there still are errors that stem from typos or ambiguities, but the bulk of the issues appear to be connected to the text's peculiarities. Simultaneously, the problematic tokens hint at there being a need for a more uniform way of determining what lemma to choose for verb-derived nouns and adjectives or words that have more than one acceptable spelling.

\renewcommand{\arraystretch}{1.25}
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf Error Type & \bf Raw Freq. & \bf Relative Freq. (\%) \\ \hline
\textit{y} & 35 & 24.14\%  \\
proper name & 30 & 20.69\%  \\
\textit{nie} & 19 & 13.10\%  \\
spelling & 12 & 8.28\% \\ 
surname & 12 & 8.28\% \\
capitalization & 8 & 5.52\% \\
abbreviation & 8 & 5.52\% \\
\textit{e} & 7 & 4.83\% \\
ambiguous & 3 & 2.07\% \\
name & 3 & 2.07\% \\
unidentified & 3 & 2.07\% \\
problematic & 2 & 1.38\% \\
foreign & 2 & 1.38\% \\
archaic & 1 & 0.69\% \\
\hline
\end{tabular}
\caption{\label{table:lemmas-errors} Types of errors and their raw and relative frequencies among the historical tokens that were mislabelled by both Stanza and Morfeusz.}
\end{center}
\end{table}

While a major drawback of this method of discovering a historical text's peculiarities is that there needs to be a gold standard to compare the performance of the lemmatizers to, it does reveal some interesting insights into the kinds of tokens that appear to be nonstandard for Polish and typical for a given text. What could be done in the case of texts with no gold standard is simply reviewing the entire output - but that would, in the long run, be almost identical to manually lemmatizing it in the first place.

% UPOS
\subsection{UPOS-tagging}
\label{subsec:upos-tagging}

As detailed in \autoref{subsec:bert-tagging}, \autoref{subsec:marmot-tagging}, \autoref{subsec:stanza-tagging}, and \autoref{subsec:ud-tagging}, the four taggers that either utilized or could be trained to utilize the UD tagset were BERT, Marmot, Stanza, and the UD Cloud tagger. Unlike in the case of lemmatization, when it comes to tagging, there is a specific number of classes in question, which allowed for the extension of the evaluation metrics from just accuracy to accuracy, weighted precision, weighted recall, and Matthew's Correlation Coefficient, which are presented in \autoref{table:upos}. Since the UD tagset is not large, precision and recall were also calculated for each class for a deeper insight into which classes are the most problematic \citep{ud-tagset}. These results can be found in \autoref{upos-class-measures}. Based on the general evaluation measures BERT performed best on both the modern and the historical dataset, while the UD Cloud tagger has the worst performance on both of the test sets. Although not by a lot, Stanza's neural pipeline outperforms Marmot. For all of the taggers, the historical dataset achieves a consistently lower score than the modern one, indicating issues that are not specific to the taggers themselves. 

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|cc|cccc|}
\hline \bf Model & \bf Data & \bf Accuracy & \bf Precision & \bf Recall & \bf MCC \\ \hline
\multirow{2}{4em}{BERT}
& PDB & 99.20\% & 99.20\% & 99.20\% & 99.08\% \\
& memoir & 94.50\% & 94.72\% & 94.50\% & 93.77\%  \\
\multirow{2}{4em}{Marmot}
& PDB & 97.73\% & 97.75\% & 97.73\% & 97.38\% \\
& memoir & 90.61\% & 90.79\% & 90.61\% & 89.30\% \\
\multirow{2}{4em}{Stanza}
& PDB & 98.40\% & 98.41\% & 98.40\% & 98.16\% \\
& memoir & 93.31\% & 93.52\% & 93.31\% & 92.43\% \\
\multirow{2}{4em}{UD Cloud}
& PDB & 90.98\% & 91.17\% & 90.98\% & 89.59\% \\
& memoir & 83.41\% & 84.12\% & 83.41\% & 81.17\% \\ 
\hline
\end{tabular}
\caption{\label{table:upos} UPOS-tagging evaluation measures (accuracy, precision (weighted), recall (weighted)), Matthew's Correlation Coefficient per model and per test data type. Although calculated, F1 is not given since it can be calculated from precision and recall. Per class precision and recall can be found in \autoref{upos-class-measures}}
\end{center}
\end{table}

Similarly as in the case of lemmatization, manual annotation of errors made by the taggers has revealed certain recurring features, as outlined in \autoref{table:pos-errors}; this time, however, the token in question did not have to be misclassified by all of the taggers, and instead only two of them had to have made a mistake while tagging a given token, as that was deemed more likely to still remove the tagger-specific issues while preserving more information on the possible problematic features of the data than if all of the taggers had to misclassify the token. A more detailed definition of the types of errors along with examples can be found in \autoref{table:error-type-upos-explanations} in \autoref{error-types}.

Similarly to the errors made during lemmatization, the errors can be divided into spelling-related and foreign vocabulary-related. However, another category appears to be taking shape here, namely that of form-related errors. These encompass a number of ambiguous, ending, problematic, and impersonal type errors, with the category of "ambiguous" being the most numerous error category in general. In that case, the word itself appears to be classified according to the wrong meaning or just by the ending, which can itself be ambiguous and lead to different interpretations. Clearly, the word form itself (or the inflectional ending itself) should not be the only factor determining a word's class; this may hint at an additional level of difficulty in terms of unusual word order if such information is utilized by the tagger. In addition, some errors stem from the UD tagging rules which are complicated e.g. when it comes to the VERB vs. AUX distinction for the verb \textit{byÄ‡} in Polish. What is worth noting is the high number of capitalization-related errors. It appears that capitalization is factored in as a feature when it comes to tagging words as PROPN - and, as a result, regular words written unexpectedly with a capital letter at the start are often misclassified as such. 

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf Error Type & \bf Raw Freq. & \bf Relative Freq. (\%) \\ \hline
ambiguous & 208 & 21.80\% \\
capitalization & 199 & 20.86\% \\
\textit{y} & 109 & 11.43\%  \\
unidentified & 62 & 6.50\% \\
archaic & 59 & 6.19\% \\
UD & 58 & 6.08\& \\
surname & 41 & 4.30\% \\
\textit{e} & 41 & 4.30\% \\
\textit{nie} & 28 & 2.94\% \\
ending & 24 & 2.56\% \\
spelling & 23 & 2.41\% \\
proper name & 21 & 2.20\% \\
problematic & 20 & 2.10\% \\
digits & 17 & 1.78\% \\
foreign & 13 & 1.36\% \\
uncommon & 12 & 1.26\% \\
abbreviation & 11 & 1.15\% \\
impersonal & 4 & 0.42\% \\
name & 2 & 0.21\% \\
currency & 1 & 0.11\% \\
special & 1 & 0.11\% \\
\hline
\end{tabular}
\caption{\label{table:pos-errors} Types of errors and their raw and relative frequencies among the historical tokens that were mislabelled by at least two of four UPOS taggers (BERT, Marmot, Stanza, UD Cloud).}
\end{center}
\end{table}

Class-specific measures yield a deeper insight into which classes in particular are more problematic. For BERT (as can be seen in \autoref{table:bert-pr}), all of the classes in the modern text, with the exception of INTJ and SYM perform very well. The two aforementioned classes could be problematic due to their scarcity. They are also not present at all in the historical data. As for the memoir, there is a noticeable drop in performance for ADV, AUX, PART, PROPN, SCONJ, and X - which are also less numerous than some of the other classes. Additionally, there is possibly some confusion between AUX and PART when it comes to the classification of the token \textit{to} `it' or `is'. Naturally, the numerous new proper nouns found in the historical text are also problematic, not to mention the issue of nonstandard capitalization. 

Marmot (\autoref{table:marmot-pr}) similarly struggles with SYM and INTJ in the modern data, but it also scores relatively lower on AUX, PART, PROPN, and X on the PDB test set. When it comes to the historical data, a noticeable drop in performance can be noticed for the ADJ category, alongside ADV, AUX, PART, PROPN, SCONJ, and X. Aside from the ADJ category (which is more complicated due to place name-derived adjectived being capitalized by the author and surnames being misclassified as adjectives), the same categories seem to be problematic for Marmot as for BERT, although in this case some of these issues are already noticeable on the modern test set.

Stanza's results (which can be found in \autoref{table:stanza-pr}) show high precision, but low recall on the problematic INTJ and SYM classes. Otherwise, the tagger performs quite well across all of the categories in the modern data, with the possible exception of relatively low recall on PART. When it comes to the historical data, a noticeable drop in performance can be observed for ADJ, AUX, PROPN, SCONJ, and X, which again mostly overlaps with the categories that were problematic for the aforementioned taggers.

The UD Cloud tagger (\autoref{table:ud-pr}) shows a much more varied performance across the classes on both the modern and historical test data. As far as the PDB test set is concerned, it struggles with ADJ, ADV, AUX, NUM, PROPN, and X classes in particular. As for the memoir, a large drop in performance can be observed for most classes. Noticeably, the tagger performs well on ADP, CCONJ, and PUNCT, and has high precision, but very low recall on DET. While the issues that this tagger has with the modern data do partly overlap with the classes that were problematic for other taggers, this tendency is not that clear in the historical data due to the overall bad performance of the tagger. However, the issues visible on the PDB test set may hint at these classes being more problematic in both modern and historical data, and perhaps the errors made while classifying those are more visible due to the classes' lower frequencies.

Although there is an overlap in terms of what features can lead to a token being misclassified, they seem to be of different importance when it comes to assigning UPOS tags compared to lemmatization. UPOS tagging does share the same issue as lemmatization when it comes to the need for some gold standard to compare the tagging to. While it does hint at certain text-specific issues, such as nonstandard spelling or unusual vocabulary, the bulk of the errors appears to stem from the ambiguity of some tokens.

% XPOS
\subsection{XPOS-tagging}
\label{subsec:xpos-tagging}

In accordance with what was described in \autoref{subsec:bert-tagging}, \autoref{subsec:marmot-tagging}, \autoref{subsec:stanza-tagging}, and \autoref{subsec:morfeusz-tagging}, the four tools used for XPOS-tagging experiments were BERT, Marmot, Stanza's neural pipeline and the combination of the morphological analyzer Morfeusz and a morphosyntactic tagger Concraft-pl. All of these tools were previously used for either lemmatization or UPOS tagging. While the same general evaluation measures as in UPOS-tagging were employed in this task, a decision was made to leave out the tag-specific measures due to the sheer number of possible classes. The possible labels for the fine-tuned BERT model, extracted from the training and test set of PDB along with the historical data, consist of 898 different classes. 

The overall performance of the tools in the XPOS-tagging task was worse than in the UPOS-tagging one. This could be attributed to a larger number of classes that require finer distinctions to be made. Same as before, all of the tools perform noticeably worse on historical data compared to modern data. The best-performing tool on both test sets is BERT and the worst is Marmot. What is worth pointing out in this case though is that Morfeusz and Concraft-pl's CRF architecture does outperform Stanza's neural pipeline slightly, but only on modern data.    

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|cc|cccc|}
\hline \bf Model & \bf Data & \bf Accuracy & \bf Precision & \bf Recall & \bf MCC \\ \hline
\multirow{2}{4em}{BERT}
& PDB & 95.65\% & 95.13\% & 95.65\% & 95.47\% \\
& memoir & 89.39\% & 89.75\% & 89.39\% & 89.05\%  \\
\multirow{2}{4em}{Marmot}
& PDB & 89.27\% & 88.95\% & 89.27\% & 88.83\% \\
& memoir & 80.22\% & 81.34\% & 80.22\% & 79.60\% \\
\multirow{2}{4em}{Stanza}
& PDB & 94.29\% & 94.25\% & 94.29\% & 94.05\% \\
& memoir & 87.68\% & 88.44\% & 87.68\% & 87.28\% \\
\multirow{2}{4em}{Morfeusz}
& PDB & 94.43\% & 95.36\% & 94.43\% & 94.20\% \\
& memoir & 84.26\% & 86.83\% & 84.26\% & 83.76\% \\ 
\hline
\end{tabular}
\caption{\label{table:xpos} XPOS-tagging evaluation measures (accuracy, precision (weighted), recall (weighted)), Matthew's Correlation Coefficient per model and per test data type. Although calculated, F1 is not given since it can be calculated from precision and recall.}
\end{center}
\end{table}

A manual error analysis and annotation has revealed a number of trends concerning the mistakes that the taggers make, as presented in \autoref{table:xpos-errors}. Definitions and examples of the errors can be found in \autoref{table:error-type-xpos-explanations} in \autoref{error-types}. A similar trend can be noticed in terms of the kinds of errors as in the UPOS-tagging task. The most numerous error type is the ambiguous errors, where one word form corresponds to multiple possible tags (e.g. in some declension paradigms certain cases have the same form). Once again, the prevalence of these errors hints either at the taggers not being able to properly utilize the contextual information that is necessary for the disambiguation of the class either due to their architecture or to the text's unusual word order; and again, the latter cannot be fully concluded simply from these results. 

One relatively prominent error category, namely digits, appears to stem from the tools utilizing different strategies when it comes to numbers written as digits. Some of them classify them as \textit{dig}, while others attempt to provide the tag as if the number were written out with letters. Another tagset-related issue is that of currency mistakes; the XPOS tagset generally divides masculine nouns into three subgenders: m1, which includes animate human nouns, m2, which includes animate non-human nouns, and m3, which includes inanimate nouns. These are supposed to reflect which form the determiner \textit{ktÃ³ry} `which' takes when referring to that noun \citep{ud-masculine-gender}. According to this, however, there are less obvious words that belong to the m2 category, such as currency names, which did seem to pose a problem for the taggers. In general, some mistakes were made when distinguishing between the m1 and m3 categories for other tokens as well, but they were not classified separately.

Finally, although not numerous, two kinds of errors are worth mentioning in the context of the data, namely the gender and vocative ones. Although the gender category partly overlaps with the aforementioned masculine subgender distinction, a number of the instances in this error category stem from the fact that not all personal pronouns in Polish reflect gender (the first and second person in both singular and plural do not have an overt marking of the gender on the pronoun itself). However, since some do, all of the personal pronouns are annotated for gender. This leads to a number of personal pronouns being misclassified as masculine, when they should, in fact, be feminine, as they are uttered by a female character and followed by feminine verb or adjective forms. As for the vocative category, when the word can be interpreted otherwise, the taggers tend to not utilize the tags for the vocative case. Both of these issues hint at the fact that the training data used for the taggers may be lacking in dialogues where both first and second-person personal pronouns and nouns in the vocative case would be more present. Additionally, the tendency to select masculine over feminine for the pronouns may hint at a potential gender bias in the data.

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf Error Type & \bf Raw Freq. & \bf Relative Freq. (\%) \\ \hline
ambiguous & 199 & 38.20\% \\
unidentified & 65 & 12.48\% \\
proper name & 52 & 9.98\% \\
\textit{y} & 39 & 7.49\% \\
digits & 25 & 4.80\% \\
problematic & 22 & 4.22\% \\
\textit{nie} & 20 & 3.84\% \\
spelling & 18 & 3.46\% \\
archaic & 17 & 3.26\% \\
foreign & 16 & 3.07\% \\
surname & 12 & 2.30\% \\
uncommon & 10 & 1.92\% \\
currency & 8 & 1.54\% \\
\textit{e} & 7 & 1.34\% \\
gender & 4 & 0.77\% \\
vocative & 3 & 0.58\% \\
abbreviation & 2 & 0.38\% \\
name & 2 & 0.38\% \\
\hline
\end{tabular}
\caption{\label{table:xpos-errors} Types of errors and their raw and relative frequencies among the historical tokens that were mislabelled by at least two of four XPOS taggers (BERT, Marmot, Stanza, Morfeusz).}
\end{center}
\end{table}

The issues that plague the XPOS taggers resemble those characteristic of UPOS tagging, with some novel types of errors being more indicative of the issues with the tagset or the training data. Once again, while this method does hint at unusual vocabulary and spelling practices being present in the text, and allows for the noticing of patterns within those trends, it requires prior manual annotation, which makes it more difficult to utilize it at a larger scale.

% ngrams - tomorrow
\subsection{n-gram statistics}
\label{subsec:ngram-stats-results}

The constructed unigram comparisons for UPOS and XPOS tags offer insight into the composition of the text, without delving into the word order or sentence structure. Due to there being over 600 different XPOS tags in the combined test sets (and even more when it comes to their combinations in bi- and trigrams), the entirety of the comparison cannot be represented within this thesis, and instead a subsection of the most popular tags will be discussed. \texttt{.xlsx} files with full comparisons, sorted by the relative frequency of an n-gram in the PDB data can be found in the thesis repository, as outlined in \autoref{app-resources}. 

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf UPOS tag & \bf \makecell[c]{PDB \% \\ frequency} & \bf \makecell[c]{memoir \% \\ frequency} \\ \hline
NOUN & \textbf{24.94\%} & 23.86\% \\
PUNCT & \textbf{16.76\%} & 11.71\% \\
VERB & \textbf{11.57\%} & 10.97\% \\
ADP & 10.49\% & \textbf{11.53\%} \\
ADJ & \textbf{10.00\%} & 9.01\% \\
PRON & 4.75\% & \textbf{4.91\%} \\
PROPN & 3.32\% & \textbf{6.83\%} \\
CCONJ & 3.26\% &\textbf{ 5.28\%} \\
ADV & 3.25\% & \textbf{3.29\%} \\
PART & \textbf{2.86\%} & 2.00\% \\
DET & 2.52\% & \textbf{4.19\%} \\
AUX & 2.50\% & \textbf{2.56\%} \\
SCONJ & \textbf{2.04\%} & 1.93\% \\
X & \textbf{0.92\%} & 0.64\% \\
NUM & 0.79\% & \textbf{1.29\%} \\
INTJ & \textbf{0.03\%} & 0.00\% \\
SYM & \textbf{0.01\%} & 0.00\% \\
\hline
\end{tabular}
\caption{\label{table:upos-unigrams} The UPOS unigram \% frequencies for the modern and historical test data. The higher relative frequency is indicated in bold.}
\end{center}
\end{table}

When it comes to the UPOS unigram frequencies, as presented in \autoref{table:upos-unigrams}, for the majority of the tags the difference is not large. The exception is punctuation, which is noticeably less numerous in the memoir. This could be caused by unusual punctuation employed by the author or differences in terms of the structure of the sentences in both of the sets. Longer sentences without many commas or very few utterances in quotation marks could account for a part of this discrepancy. On the other hand, the memoir features proportionally more PROPN items than the PDB test set. As detailed in the other subsections of this section, these tokens do account for a non-negligible part of what sets the historical text apart from modern ones, both in terms of vocabulary differences and issues with lemmatization and tagging. While the prevalence of PROPN is more likely due to the genre or the topic of the memoir, higher relative frequencies of CCONJ and DET in the memoir seem to indicate something related more to the language or the style of the text. The frequency of CCONJ may indicate, along with the lower relative number of punctuation, that the author prefers long sentences the components of which are connected using coordinating conjunctions, instead of splitting them into shorter sentences. As for DET, it is important to note that while Polish does not have a proper article system, DET is used to denote demonstrative pronouns and possessive pronouns. While this could partly be explained by the topic and the fact that within a coherent text like the memoir the author may, for example, refer more to previously mentioned entities (which would warrant using demonstrative pronouns), it could also, to an extent, be simply typical for the author's language.

As mentioned before, bigram and trigram frequency lists are too large to be included in the thesis, possibly even in the appendix. They can be accessed in the form of \texttt{.xlsx} files in the repository included in \autoref{app-resources}. Nevertheless, a discussion of them is warranted, as, unlike unigrams, they may hold clues to variation in word order and syntax. Since the relative frequencies provided for bigrams situate them in the general collection of all the bigrams, it is not possible to compare them directly within one subcategory (meaning that the frequencies of all the bigrams starting with <BOS> in the historical data do not add up to the same fraction as in the modern data).

Regardless of that, certain interesting trends can be noted. Overall, there appear to be fewer bigrams starting with <BOS> in the historical data, which would support the idea that the text consists of longer sentences than the PDB test set. Despite that, in the historical data, the <BOS> CCONJ bigram is still relatively more numerous, which appears to be a peculiarity of the author's language, perhaps indicating a more informal style of writing. There is also a large discrepancy when it comes to <BOS> PUNCT, this time in the favor of PDB, also indicating that there are more examples in that set that are marked as utterances in a dialogue or quotes. The historical data seems to feature more ADJ ADJ bigrams, which could indicate the omission of a comma that normally separates those. Relatively, the modern text features more ADJ NOUN bigrams than the memoir; it also has a higher relative count of NOUN ADJ bigrams, which indicates that a comparison against the whole selection of bigrams and not just within a subcategory can be less informative, as one needs to factor in the frequency of the constituent tags themselves. 

The historical text seems to feature more ADP PROPN bigrams, which is reasonable given the prevalence of the PROPN tag in the memoir. There appears to be some variation in terms of the bigrams starting with AUX, with a surprising disparity in favor of PDB when it comes to the AUX VERB bigram; this is especially noticeable given the fact that proportionally the counts for AUX and VERB separately are not very different in both test sets. AUX appears to be followed more often by ADP or NOUN in the historical data. The DET NOUN and DET PROPN combinations are noticeably more frequent in the historical data than in the modern data, but there is no large difference between the frequencies for DET ADJ for the two sets. Similarly, the historical data shows proportionally many more NOUN DET combinations, hinting at a possible word order variation. The same can be noticed for PROPN DET and DET PROPN. While verbs seem to be more often directly followed by nouns in the historical data, it is impossible to determine whether that is due to some word order inversion or simply because of the object immediately following the verb. There is a small number of bigrams that only appear in one of the sets of data, partly due to INTJ and SYM only being present in the PDB data.

As far as trigrams are concerned, many more combinations are available, and, unfortunately, not all of them can be discussed within this subsection, so a special focus will be put on the trends that were already previously noticed, such as the word order in adjectival phrases. The ADJ DET NOUN and ADJ NOUN DET trigrams appear more common in the historical data. DET ADJ NOUN appears to be equally frequent, while DET NOUN ADJ and NOUN DET ADJ are again more common in the historical data. While these results may be hinting at a tendency for unusual word order in certain phrases, this method does not allow for the distinction of whether the ADJ and DET elements were actually modifiers for the NOUN token. While this could be more noticeable in XPOS bigrams and trigrams, the number of possible combinations makes it impossible for any more in-depth analysis of those results without further computational processing, which is why the discussion of the results obtained for XPOS bigrams and trigrams is omitted (while the XPOS unigrams are discussed below).

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|cc|}
\hline \bf XPOS tag & \bf \makecell[c]{PDB \% \\ frequency} & \bf \makecell[c]{memoir \% \\ frequency} \\ \hline
interp & \textbf{16.77\%} & 13.56\% \\
part &\textbf{ 4.74\%} & 3.48\% \\
conj & 3.26\% & \textbf{4.98\%} \\
fin:sg:ter:imperf & \textbf{3.00\%} & 0.61\% \\
subst:sg:gen:f & \textbf{2.51\%} & 1.71\% \\
prep:loc:nwok & \textbf{2.47\%} & 1.65\% \\
comp & 2.04\% & \textbf{2.26\%} \\
subst:sg:nom:m1 & 1.92\% & \textbf{4.56\%} \\
subst:sg:nom:f & 1.88\% &\textbf{ 2.17\%} \\
prep:loc & \textbf{1.71\%} & 1.37\% \\
\hline
\end{tabular}
\caption{\label{table:xpos-unigrams} The top 10 XPOS unigram \% frequencies for the modern and historical test data. The higher relative frequency is indicated in bold. The full comparison of unigrams can be found in \autoref{app-resources}.}
\end{center}
\end{table}

While due to the sheer number of the XPOS tags appearing in the test data an equally detailed analysis cannot be conducted, certain interesting trends can be identified. Once more, punctuation appears on the top of the frequency table, with the PDB test set having proportionally more such tokens. What is interesting is that the PDB test set features significantly more verbs in the present tense (fin) as opposed to the memoir, in which past forms (praet) prevail. This is not unexpected given the genre of the memoir, but it could also hint at certain forms being underrepresented in the PDB. The prevalence of tokens marked as subst:sg:nom:m1, which refers not just to masculine, but masculine animate human nouns, indicates a certain bias in the memoir, where male characters were given more spotlight by the author. Looking at the full data, contrary to the expectations, vocative forms do not appear to be proportionally severely underrepresented in the PDB, but they are not frequent in general, which may lead to issues with detecting those forms by tools trained on the PDB. There is a number of tags that only appear in one of the test sets; their total number is still smaller than that of the classes used for training the BERT model (based on the test sets and the PDB training set). However, it is possible that even this collection of tags is incomplete given the number of possible feature combinations.

While this method does reveal distributional differences, its usefulness is mostly constrained to unigrams and UPOS bigrams, as more processing of the results would be needed for data where there are thousands of n-gram combinations. Additionally, while presenting the bigram or trigram frequency as a fraction of the whole set of n-grams does allow for some comparisons, representing the frequency within the category of bigrams that start or end with a given element could yield additional insights. The lack of explicit indication as to what relation the elements in an n-gram have to each other makes this method inferior to the one employed by \citet{johannsen-etal-2015-cross}, although the annotation required for it may be less time-consuming. Nevertheless, since an annotation effort already has to be made, including syntactic relations in the annotation and then conducting an analysis of the syntactic treelets should yield clearer results.  

% nkjp
\subsection{NKJP vocabulary comparison}
\label{subsec:nkjp-comparison-results}

As discussed in \autoref{subsec:nkjp-vocab}, a comparison of the unique tokens and lemmas from both of the test sets against a subsection of the data from the National Corpus of Polish was conducted. As visualized in \autoref{table:nkjp-results}, noticeably more tokens and lemmas from the historical data were not found in the National Corpus of Polish when compared to the results from the PDB test set. The difference is statistically significant. What is worth noting is that while PDB is based on the texts from the National Corpus of Polish, it does include some sentences that do not come from that corpus and can therefore include tokens and lemmas that are not present in the corpus. 

\renewcommand{\arraystretch}{1.25}
\begin{table}[H]
\begin{center}
\begin{tabular}{|cc|c|c|c|}
\hline \bf Data & \bf Data & \bf Total unique & \bf Not found & \bf \% \\ \hline
\multirow{2}{4em}{PDB}
& lemmas & 7583 & 44 & 0.58\%  \\
& tokens & 12601 & 56 & 0.44\%  \\
\multirow{2}{4em}{Historical}
& lemmas & 1213 & 86 & 7.09\%  \\
& tokens & 4302 & 346 & 8.04\% \\ 
\hline
\end{tabular}
\caption{\label{table:nkjp-results} Raw and \% numbers of tokens and lemmas unique to the modern or historical test sets when compared with a subset of the NKJP.}
\end{center}
\end{table}

An inspection of the items that were returned as having no matches in the National Corpus of Polish for the PDB test set (which can be found in \autoref{app:nkjp}) reveals that many of them overlapped across the lemma and token category (i.e. if a token was not found in the corpus, neither was its lemma and vice versa). A number of the missing items include punctuation, which appears to not be included in the corpus searches, as well as different numbers and numerals, which, due to their practically infinite number, is understandable. Additionally, a number of place names, names, surnames, and brand names were not found in the NKJP. Finally new borrowings with nonstandard spelling reflecting the original pronunciation (the diminutive \textit{lajwik} `live (stream)') as well as highly specialized vocabulary (\textit{trichlorobenzen} `trichlorobenzene') were not found in the National Corpus of Polish either, which is warranted as these are either new or very rarely used words.

A similar trend in terms of both the token and the lemma missing from the National Corpus of Polish can be noticed for the historical data, although here the comparison is more difficult, as the unique tokens were extracted from a larger text sample than the unique lemmas (which had to be manually annotated). Once again, some punctuation is listed as not found due to the search engine's limitations. However, almost no standard numbers or numerals are listed as not found, with the exception of \textit{cwansiger} `20 (coin/bill)', which appears to be a borrowing from German. Similarly as with modern data, a large number of proper names and surnames was not found in the corpus. A large part of the vocabulary with no hits in the NKJP consists of either words spelled in a nonstandard fashion (with the spelling of \textit{nie} together with the verb it modifies and the use of the grapheme \textit{y} for /j/ being the most prominent; there is one interesting instance of the sound \textipa{/\textrtailz/} being spelled as \textit{rÅ¼}, while the two accepted modern spellings are \textit{rz} and \textit{Å¼}\footnote{These two spellings reflect a past phonemic merger.}), or of words that appear to be out of use or highly specific to the sociohistorical context of the text (such as \textit{mandatariat} `the position of being a potentiary' or \textit{mortyfikowaÄ‡} `to self-flagellate'), with a potential overlap between the two categories. What is also noticeable is that some of the words reappear with multiple variations of spelling (e.g. \textit{jurysdykcya}, \textit{juryzdyksya} `jurisdiction' or \textit{mandataryusz}, \textit{mandatyrusz} `potentiary') indicating a certain degree of inconsistency when it comes to spelling; simultaneously, when it comes to tokens, and not lemmas, some words reappear with the same nonstandard spelling but various inflectional endings.

A simple comparison of the vocabulary of a given text or dataset to that of a large corpus of a given language appears to yield informative results as far as those texts' divergence from the standard is concerned. With the exclusion of terms that are naturally unlikely to appear in the corpus (surnames, proper names), there still remain tokens and lemmas that were not identified in the corpus due to their spelling or rarity, and some of the same conclusions as to the nature of these differences can be drawn here as from the experiments \autoref{subsec:lemmatization}, \autoref{subsec:upos-tagging}, and \autoref{subsec:xpos-tagging}. While this method does not return as many items that were not found (or, in the case of the taggers, errors), it only requires the text to be lemmatized, not annotated for the part of speech - and even simply searching for the tokens, and not their lemmas, yields interesting results that are not extremely different from those for lemmas. Overall, this kind of comparison appears to be quite rewarding for the amount of preprocessing or annotation required.