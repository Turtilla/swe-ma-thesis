\section{Ethical Considerations}
\label{sec:ethicalcons}

Ethical concerns are ever-present within the field of Natural Language Processing. As \citet{hovy-spruit-2016-social} point out, these concerns can revolve both around the data itself and the impact that NLP can have on society. \citet{stochastic-parrots} point out the environmental impact of computationally-heavy processes (such as training very large language models) and draw the readers' attention to how biases existing in the training data can impact the aforementioned models. A number of different tools and resources were utilized in this thesis, and many of them deserve to be discussed from an ethical point of view.

To begin with, the experiments conducted as a part of this thesis did not involve training any large models from scratch --- and the most computationally expensive part was the fine-tuning of two BERT-based part-of-speech taggers. While training a large transformer model like BERT is definitely impactful, its ability to be fine-tuned for different applications eliminates the need to train another costly model from scratch. Utilizing pre-existing, optimized code for token tagging suited for this model likely streamlined the process as well. With the exception of Marmot, the training of which is not computationally expensive, the other taggers were already pre-trained, which made this investigation much more justifiable than training many models from the start would be with the environmental impact in mind. 

While a lot has been written about different biases in NLP, \citet{blodgett-etal-2020-language} find that many such discussions are "vague, inconsistent, and lacking in normative reasoning." They adopt a division of biases into allocational, meaning ones where the system bias distributes some resources unfairly to some social groups, and representational, where some groups are misrepresented or omitted by the system. The authors present recommendations for future work with bias in NLP, and while some of them are not relevant to this thesis, their suggestion to explicitly state what behaviors and what kinds of biases exist in the system or the data, how they could be harmful, and to whom, is of high importance. One instance of a representational bias displayed by the tools tested in this thesis has been described in \autoref{subsec:xpos-tagging}. According to the UD XPOS annotation, the first- and second-person singular pronouns are annotated for gender despite not overtly displaying it. During the tagging process, the tools did annotate pronouns used by a female speaker as masculine. This kind of a system behavior could be harmful if displayed at a larger scale, as it disregards the presence of women as speakers --- depending on what this tagging is used for in a downstream task, this could lead to e.g. a dialogue system addressing its interlocutor using incorrect pronouns and forms. 

Biases in large language models and other NLP tools do not necessarily stem from the code itself. As \citet{garimella-etal-2021-intelligent} note, "unstructured data often contain several biases, and natural language processing (NLP) models trained on them learn and sometimes amplify them." It is therefore important to discuss the kinds of data used in this thesis and whether or not they can contain such biases (potentially leading to the aforementioned gender representation bias). The author of the Polish version of BERT, \citet{k≈Çeczek_2021}, points out that the data used to pre-train his models may include biases and stereotypes, and, consequently, these could be visible in downstream tasks. While the National Corpus of Polish is claimed to be balanced and representative of the language at large, its creators do not address the issue of biases, and one can assume that certain prejudices can be reflected in the texts that constitute the corpus \citep{nkjp}. While, as \citet{wroblewska-2018-extended} explains, the Polish Dependency Bank is largely based on the National Corpus of Polish, it does include sentences from other sources and a similar conclusion can be drawn about this dependency bank as about the National Corpus of Polish. As far as the historical data discussed in this thesis is concerned, it is likely to contain biases, as it only comes from one author. As pointed out in \autoref{subsec:ngram-stats-results}, in comparison with the PDB, the author of the memoir uses proportionally more masculine animate human nouns, potentially leading to an over-representation of men at the cost of other genders, a bias that could be amplified had this data been used to train a tool for later use.  

Another tangentially related issue worth considering in the light of this thesis is the representation of small languages or dialects and the regional and diachronic variation in NLP. \citet{mcenery-etal-2000-corpus}, \citet{soria-etal-2016-fostering}, as well as  \citet{hovy-2018-social} point out that the lack of corpus data for such languages severely impedes the development of appropriate NLP tools, which can, in turn, lead to some social groups being excluded from utilizing such tools or result in their language becoming more endangered. While the data analyzed in this thesis is not a sample of a currently spoken modern minority language or dialect, some of the methods tested in the experiments could be used for exploring contemporary language variation as well, potentially contributing to solving this issue. 

Finally, while both the out-of-context sentences provided in PDB and the limited access that users have to the texts that constitute the National Corpus of Polish are methods for dealing with copyright and privacy issues, working with an independently transcribed and annotated text may pose its own ethical problems. However, in the case of this thesis, the data in question is historical, and its author passed away around a century ago, which largely voids the issue of the author's consent for the use of his text. 