\section{Experimental Setup}
\label{sec:exp-setup}

This section contains both a description of the data and the experiments that were conducted. The entirety of the code, alongside unannotated and annotated data is provided in \autoref{app-resources}. The results of the experiments are presented and discussed in \autoref{sec:results}.

\subsection{Data}
\label{subsec:data}

The data used in the experiments originates from a memoir penned by Juliusz Czermiński in 1899 in Rzeszów. The original manuscript is preserved in the collection of Zakład Narodowy im. Ossolińskich (also known as Ossolineum) with the signature 15374/II, according to the library's catalogue, but cannot be accessed digitally \citep{ossolineum}. At some point in the past, typewriter copies of the manuscript have been made and distributed among the author's descendants. In the recent years, one of them, Piotr Kociat\-kiewicz, undertook the effort of copying over the text into a Word file, and it is this digitalized data that was used throughout the thesis. Unfortunately, due to the time constraints and the physical difficulty of accessing the manuscripts, no assessment of the quality of the transcription could be made.

As mentioned before, the data originates from one author and belongs to the genre of memoir. The author was a native of an area that encompasses nowadays south-eastern Poland and western Ukraine, but was not independent at that time. From what can be gathered from the contents of the memoir, he considered himself to be Polish and wrote in an idiolect closely resembling the Polish language. However, due to the text's age and region of origin, it is likely that it diverges from standard modern Polish with regard to spelling (from which pronunciation may be inferred), grammar, and vocabulary. This assumption is strengthened by the fact that following the periodization of the history of Polish as outlined by \citet{długosz-kurczabowa_dubisz_2006}, the text could be classified as an example of writing in "early" New Polish (npol. 1.), which diverges from Modern or "late" New Polish (npol. 2.).
%% maybe this should be written elsewhere? but where?

Both the relative understandability of the text to a native speaker of Polish and the potential for it to differ from standard Polish make it a good candidate for inquiries into how such possible differences could be identified computationally.

The entirety of the memoir consists of 37405 tokens, according to Word's word count functionality. Out of those, the first 360 sentences, corresponding to 10.286 tokens, were manually annotated with UPOS (universal part of speech) tags, and the first 115 sentences, corresponding to 3271 tokens, were additionally annotated with XPOS (language-specific part of speech) tags and lemmas following the tagsets used by \citet{wroblewska-2018-extended} in PDB (Polish Dependency Bank); this decision was made due to the accessibility and universality of these tagsets, and because the results could then be compared to PDB's test set. The details of the annotation process are discussed in \autoref{subsec:annotation}. While this means that only roughly more than a quarter of the text was annotated with the UPOS tags and less than a tenth with the XPOS tags and lemmas, the annotation of the entire text was deemed to be beyond the scope of this thesis, especially given the complicated nature of the XPOS tags and the fact that the the annotation had to be of high quality. Additionally, small test samples are not unheard of when it comes to tagger-related experiments using historical data \citep{bollmann-2013-pos, hupkes16, rayson07}. 

Simultaneously, most of the procedures described in the subsequent subsections were also conducted on the PDB (Polish Dependency Bank) corpus, so that the results could be compared to those obtained on modern data; additionally, some of the taggers were locally trained on the PDB train set. As mentioned before, PDB is the largest UD-style treebank available for Polish. It features 22.152 sentences consisting of a total of 347.377 tokens, annotated according to the UD guidelines in the CoNLL-U format. Aside from the lemmas, UPOS, and XPOS tags, which are utilized in this project, the treebank also features an annotation of syntactic relations between the elements of the sentences \citep{wroblewska-2018-extended, universaldependencies}.

In the initial stages of the project, there was an idea to compare the results obtained from the discussed data with results from running the same experiments on a subset of the Korba Corpus, also known as The Electronic Corpus of 17th and 18th c. Polish Texts (up to 1772) \citep{korba}. Although code allowing for the extraction of the desired data from the corpus files was developed, it was later discovered that not only does the corpus not include UPOS tags, but its XPOS-like tags differ in minute but relevant ways from the ones used in PDB. Finding a way to unify these tagsets was deemed to be beyond the scope of this thesis and the Korba Corpus was not used in later experiments. 

\subsection{Data Annotation}
\label{subsec:annotation}

The process of data annotation occured in a number of steps. First, the data was converted from a .docx file to a .txt file and segmented so that every line corresponded to a paragraph or a section in the original text. This served as a basis for the first major step in the annotation, namely the manual annotation of a selected subsection of the text with UPOS tags. Subsequently, Python code in the form of a Jupyter Notebook that allowed for the pre-tagging using the Morfeusz morphological analysis tool \citep{kie:wol:17:morf} in tandem with Concraft-pl \citep{waszczuk-2012-harnessing, waszczuk2018morphosyntactic}, a morphosyntactic tagger which relies on Morfeusz's analyses was developed. This was used for pre-annotating the subset of the data that was intended to be annotated with XPOS tags and lemmas, as those were the types of annotation provided by Morfeusz and Concraft-pl. The results, along with the UPOS tags, were outputted into a .conllu file which adhered to the standards of that format. This pre-annotation was then manually reviewed and corrected wherever necessary.

As mentioned in \autoref{subsec:data}, the tagset used for this task was the same as the one used in the Polish Dependency Bank, the largest of the UD-standard treebanks for Polish \citep{wroblewska-2018-extended}. That was also the corpus that was consulted in problematic cases; whenever necessary, an online dictionary of the Polish language was consulted as well \citep{pwn_n.d.}.

Each type of tagging (lemma, UPOS, XPOS) was characterized by its own difficulties. When it comes to manual lemmatization - the task that would appear to be the easiest, at least to a native speaker - the issue was deciding what lemmas to enter for words that were spelled in an unconventional way. A number of the words in text were spelled together in ways that are not permitted by standard Polish; other words were simply spelled using a different spelling convention or in a way that possibly reflected pronunciation. A decision was made to preserve these peculiarities, while simultaneously trying to present the word in its base form. This was done in order to preserve the original spelling of the words and reflect how the author would have written the base form of the word. In addition, in one of the experiments which consisted of comparing the vocabulary of the text with that of a modern Polish corpus (discussed in more detail in \autoref{subsec:nkjp-vocab}), preserving the original spelling was essential, as one of the goals of the comparison was to determine if words and lemmas with that spelling occur in the corpus. It is, nevertheless, important to note that this decision may have negatively impacted the performance of some lemmatization tools during evaluation. 

UPOS tags, which not only reflect the approximate word class, but sometimes also the role of a word in the sentence, have proven to generally be rather straightforward to assign. Nevertheless, there were some instances of words that could be classified as more than one class without a straightforward way to differentiate between those two supposed meanings. One such example is the word \textit{około} 'around,' which could be classified as either an adposition or a particle in the treebanks - and for which the Dictionary of the Polish Language provided two practically identical definitions, that did not allow for an easy distinction between the two \citep{okolopwn}. Another problematic category was the rule that verbs normally treated as auxiliaries should be classified as regular verbs in purely existential sentences \citep{polishud}. 

Finally, the XPOS tags required the most attention during the review of the preannotation, predominantly due to the fact that oftentimes they include a lot of information about the features of the token, such as gender, number, aspect, etc. Consequently, there was not much room left for token-level ambiguity, but issues stemming from syntactic ambiguity persisted. Another major issue throughout the annotation process was determining whether a verb-derived word should be classified as a gerund/participle or as a noun or adjective. For example \textit{bombardowanie} 'bombing' could be treated either as a noun or as a gerund of the verb \textit{bombardować} 'to bomb.' If the word was attested for in PDB, it was tagged in the same fashion as in the corpus. Otherwise, the decisive factor was its presence as an independent word in a dictionary.  

\subsection{Experiment 1: BERT POS-tagging}
\label{subsec:bert-tagging}

The first experiment consisted of fine-tuning a BERT model for a token classification task. Being able to fine-tune one's own model was beneficial, as one was in full control of the data and tagset used in the process. However, that cannot be said for the data utilized in the training of the original BERT model. 

The fine-tuning and evaluation were conducted using the code and instructions provided in the Transformers library for Python in \texttt{transformers/examples/legacy/token-classification/}, with minor changes meant to adapt the procedure to the provided data  \citep{wolf-etal-2020-transformers}. Preprocessing of both the training, evaluation, end testing data was modified to include another script, \texttt{preproc\_bert.py}, which removed the lines required by the CoNLL-U format for Polish for non-split tokens. These lines do not feature any annotation, but indicate the range of agglutination. Therefore, they were irrelevant for the tagging, and could actually be disruptive if left in the text. No other major changes were made to the settings of the fine-tuning, as the goal of this experiment was not to find the best hyperparameters for the task, and the suggested hyperparameters were assumed to be acceptable.

The model used as a basis for the fine-tuning was \texttt{bert-base-polish-cased-v1} by \citet{kłeczek_2021}. While both the cased and uncased version of the model perform well on different evaluation tasks, according to the author the cased model features improvements over the uncased one. Additionally, due to the historical data featuring unconventional capitalization and many proper names, it was deemed relevant to maintain the capitalization.

A total of two models were fine-tuned, one for UPOS-tagging, and one for XPOS-tagging. They were trained, evaluated and tested on the PDB data. Subsequently, both of the models were tested on the historical data, which was pre-processed in the same fashion as the PDB data. The results were automatically saved in .txt files. Although this process did output a number of evaluation measures, for the sake of comparability, those were recalculated in a Jupyter Notebook file using functions from \texttt{functions.py}, a Python file containing functions used across a number of different experiments, both for the modern and historical test set. .xlsx files containing all of the annotations and only the erroneous ones were created for later analysis. 

\subsection{Experiment 2: Marmot POS-tagging}
\label{subsec:marmot-tagging}

The next experiment similarly consisted of training a tagger architecture on PDB data. In this case the tagger was a CRF-based framework called Marmot \citep{mueller-etal-2013-efficient}. Although Marmot does have pre-trained models for Polish, their tagsets did not appear to be compatible with the one used in this thesis. Therefore, a new model was trained on the PDB train set, and tested on both the PDB test set and the historical data. Similarly as in \autoref{subsec:bert-tagging}, this data had to be preprocessed using \texttt{preproc\_bert.py}. Marmot can be trained to tag both UPOS and XPOS simultaneously, so only one model was trained. 

Marmot does not output any evaluation measures, so the results were imported into a Jupyter Notebook and the necessary measures were caluclated there. Same as before, the results were also output in the form of two .xlsx files, one for all the results and one including just the mistakes made by the tagger.

\subsection{Experiment 3: Stanza POS-tagging and lemmatization}
\label{subsec:stanza-tagging}

Another tagging service that was used to annotate the historical data was that provided by Stanza \citep{manning-etal-2014-stanford, qi2020stanza}. Stanza's pipeline provides all three desired functionalities: lemmatization, XPOS annotation, and UPOS annotation. The default package for the Polish language in Stanza is based on PDB, which was extremely convenient as the tagsets were certain to match if no changes were introduced while constructing the package. In order to obtain the annotations, the Stanza pipeline was run in a Jupyter Notebook environment on both the modern and historical data. Measures compatible with those from other experiments were output for every category, and .xlsx files containing all of the annotation and the errors were produced for later comparison.

\subsection{Experiment 4: Morfeusz POS-tagging and lemmatization}
\label{subsec:morfeusz-tagging}

\subsection{Experiment 5: UD Cloud POS-tagging}
\label{subsec:ud-tagging}

\subsection{Experiment 6: n-gram statistics}
\label{subsec:ngrams}

\subsection{Experiment 7: NKJP vocabulary comparison}
\label{subsec:nkjp-vocab}

\subsection{Tagging and lemmatization error annotation}
\label{subsec:error-annotation}