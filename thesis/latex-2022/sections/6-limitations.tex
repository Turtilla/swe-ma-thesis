\section{Critiques and Limitations}
\label{sec:critiques}

While certain limitations of this thesis project have already been mentioned in previous sections, it is worthwhile to summarize them and discuss other potential issues. The first few of those pertain to the data itself, as outlined in \autoref{subsec:data}. While the original manuscript is available physically at a library in Poland, the version of the data utilized in this project has been first transcribed from the manuscript on a typewriter, and then from the typewriter version into a \texttt{.docx} file. This introduces possibilities for transcription errors, making the data less reliable than if it were transcribed directly from the manuscript. 

Another kind of limitation related to the data could be that it only comes from one author and is not very large. However, the aim of this thesis was to explore this particular memoir, not the entirety of late 19\textsuperscript{th}-century Polish. One issue connected with this is that, consequently, the genre of the memoir and the texts featured in the PDB are not necessarily the same, and some of the differences may stem not from language variation at large, but from this genre mismatch. While it is relevant to keep in mind that this data is not very representative, this limitation does not necessarily invalidate the thesis project.

Finally, the project would have greatly benefitted from all the experiments being run on an additional sample of older historical data. Unfortunately, due to differences in the tagsets, the Korba Corpus could not be used. While results for historical data do exist for one of the taggers, this, unfortunately, meant that the other tools' performance on the memoir in question could not be compared to older data, and thus the effectiveness of the methods could not be judged based on such a comparison.

As far as the annotation is concerned (\autoref{subsec:annotation}), the limited time and womanpower available meant that only a part of the data could be annotated. Ideally, this annotation should be proofread by another trained native speaker, but, unfortunately, that was not possible within the given timeframe. This is likely to have negatively impacted the quality of the annotation. Additionally, as discussed in \autoref{subsec:ngram-stats-results}, including the syntactic dependency annotation could have yielded more interesting results; once again though, the choice to omit this annotation was made due to the aforementioned reasons. On the topic of annotation, one of the limitations of many of the discussed methods is that they require the historical data to already be annotated. However, those methods could also point in the direction of what kind of pre-processing would be needed for a more reliable automated annotation of historical data.

While the performance of BERT-based taggers was outstanding, it is possible that it could have been better. As described in \autoref{subsec:bert-tagging}, the hyperparameters used to fine-tune the taggers were the ones suggested by default by the authors of the tagging framework. It is possible that these were not optimal for the task.

The error annotation (\autoref{subsec:error-annotation}) conducted in this project is largely subjective, and some of the categories are partly overlapping. Some tokens could likely be classified as more than one category. It is important to keep in mind that this annotation was not intended to yield a strict measure but to offer more general insights into what kinds of tokens are the most problematic for the taggers and lemmatizers.

Finally, as discussed in more detail in \autoref{subsec:ngram-stats-results}, the time limitations made it impossible to analyze and discuss all of the obtained n-gram results. The number of possible XPOS tags is staggering, and the decision to construct n-grams out of them without clustering them into more general categories resulted in an output that was too large to analyze in the allotted time.