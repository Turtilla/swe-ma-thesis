{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b446bea7",
   "metadata": {},
   "source": [
    "# KORBA DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3677803",
   "metadata": {},
   "source": [
    "### IMPORTS, VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d222c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "random.seed(3)\n",
    "path = '../data/korba_corpus/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b711ad5",
   "metadata": {},
   "source": [
    "### FUNCTIONS AND CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f54354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_corpus_data(path, select_subcorpus=True, subcorpus_size=50):\n",
    "    '''A function that retrieves the relevant elements of the annotation from the corpus files.\n",
    "    \n",
    "    Args:\n",
    "        path (str): The name of the directory that all the corpus files are stored in.\n",
    "        select_subcorpus (bool): Decides whether a randomized subcorpus should be selected from all the data.\n",
    "        subcorpus_size (int): Decides how many random files should be read in if select_subcorpus is true. \n",
    "        \n",
    "    Returns:\n",
    "        A list of dictionaries representing words and their annotations.\n",
    "    '''\n",
    "    # retrieving all the xml elements for every 'seg' element\n",
    "    ann_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if len(dirs) > 0:\n",
    "            if select_subcorpus:\n",
    "                random.shuffle(dirs)\n",
    "                dirs = dirs[:subcorpus_size]\n",
    "            for i, directory in enumerate(tqdm(dirs, desc='Loading files...')):\n",
    "                new_path = os.path.join(path, directory, 'ann_morphosyntax.xml')\n",
    "                tree = ET.parse(new_path)\n",
    "                entry_holder = []\n",
    "                for elem in tree.iter():\n",
    "                    if elem.tag == '{http://www.tei-c.org/ns/1.0}seg':\n",
    "                        if len(entry_holder) > 0:\n",
    "                            ann_list.append(entry_holder)\n",
    "                            entry_holder = []\n",
    "                    else:\n",
    "                        entry_holder.append((elem.attrib, elem.text))\n",
    "                ann_list.append(entry_holder)\n",
    "    \n",
    "    # cleaning up the annotations for every word\n",
    "    new_list = []\n",
    "    for i, elem in enumerate(tqdm(ann_list, desc='Cleaning up annotations...')):\n",
    "        new_elem = {}\n",
    "        for j, element in enumerate(elem):\n",
    "            if 'type' in element[0]:\n",
    "                continue\n",
    "            elif 'name' in element[0]:\n",
    "                # orth is a \"corrected\" spelling version - not relevant in this case\n",
    "                #if element[0]['name'] == 'orth':\n",
    "                    #value = elem[j+1][1]\n",
    "                    #new_elem['orth'] = value\n",
    "\n",
    "                # word form\n",
    "                if element[0]['name'] == 'translit':\n",
    "                    value = elem[j+1][1]\n",
    "                    new_elem['translit'] = value\n",
    "\n",
    "                # possible lemmas\n",
    "                # elif element[0]['name'] == 'base':\n",
    "                    # value = elem[j+1][1]\n",
    "                    # new_elem['base'] = value\n",
    "\n",
    "                # ctag is not a UPOS tag so it is not as relevant\n",
    "                #elif element[0]['name'] == 'ctag':\n",
    "                    #value = elem[j+1][0]['value']\n",
    "                    #new_elem['ctag'] = value\n",
    "\n",
    "                # xpos tag\n",
    "                elif element[0]['name'] == 'interpretation':\n",
    "                    value = elem[j+1][1]\n",
    "                    if \"::\" in value:\n",
    "                        lemma = ':'\n",
    "                        tag = value[2:]\n",
    "                    else:\n",
    "                        lemma = value.split(':')[0]\n",
    "                        tag = \":\".join(value.split(':')[1:])\n",
    "                    new_elem['base'] = lemma\n",
    "                    new_elem['interpretation'] = tag\n",
    "\n",
    "        new_list.append(new_elem)\n",
    "\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1214dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corpus_data(corpus_list, outfile, xpos_outfile):\n",
    "    '''A function that further processes and saves the annotations to two files, one with both lemmas and xpos, one with just\n",
    "    xpos tags.\n",
    "    \n",
    "    Args:\n",
    "        corpus_list (list[dict]): A list of dictionaries produced by exctract_corpus_data() containing words and their annotation.\n",
    "        outfile (str): The name of the file to save all the annotations to.\n",
    "        xpos_outfile (str): The name of the file to save the tokens and xpos annotations to.\n",
    "    '''\n",
    "    data_towrite = []\n",
    "    xpos_towrite = []\n",
    "    excluded = []\n",
    "    for element in corpus_list[1:]:\n",
    "        if len(element) == 0:\n",
    "            data_towrite.append('\\n')\n",
    "            xpos_towrite.append('\\n')\n",
    "        elif len(element) == 3:\n",
    "            word = element['translit']\n",
    "            if ' ' in word:\n",
    "                word = word.replace(' ', '')\n",
    "            data_towrite.append(' '.join([word, element['base'], element['interpretation']])+'\\n')\n",
    "            xpos_towrite.append(' '.join([word, element['interpretation']])+'\\n')\n",
    "        else:\n",
    "            continue\n",
    "            # There are elements without an interpretation - they are those that were originally misparsed (not by reading\n",
    "            # in the .xml files but when the corpus was created), or so it would appear.\n",
    "\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.writelines(data_towrite)\n",
    "    with open(xpos_outfile, 'w') as f:\n",
    "        f.writelines(xpos_towrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43debfe0",
   "metadata": {},
   "source": [
    "### EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15986231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files...: 100%|█████████████████████████████████████████████████████████████████| 50/50 [00:04<00:00, 10.13it/s]\n",
      "Cleaning up annotations...: 100%|█████████████████████████████████████████████| 30345/30345 [00:00<00:00, 153074.58it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_list = extract_corpus_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f9d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_corpus_data(corpus_list, '../data/korba_clean.txt', '../data/korba_clean_xpos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792657f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
