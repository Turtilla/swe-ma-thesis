{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0206f6f4",
   "metadata": {},
   "source": [
    "# MORFEUSZ & CONCRAFT - PREANNOTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48b099",
   "metadata": {},
   "source": [
    "### IMPORTS, VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c688d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kawu/concraft-pl\n",
    "# https://github.com/kawu/concraft-pl/tree/master/bindings/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab283a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "sys.path.append('../concraft-pl/bindings/python/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5247dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from morfeusz2 import Morfeusz\n",
    "from concraft_pl2 import Concraft, Server\n",
    "\n",
    "server = Server(model_path='../concraft-pl/model-SGJP.gz')\n",
    "file = '../data/memoirs_annotated_10k.txt'\n",
    "original_file = '../data/memoirs_10k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0012aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "morfeusz = Morfeusz(expand_tags=True)\n",
    "concraft = Concraft()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4643c6d",
   "metadata": {},
   "source": [
    "### FUNCTIONS AND CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8d3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_interpretation(dag_disamb: list):\n",
    "    '''A function that allows for the selection of only the best possible morphosyntactic interpretation of a sentence as\n",
    "    returned by Morfeusz2 + Concraft.\n",
    "    \n",
    "    Args:\n",
    "        dag_disamb (list): A list of possible interpretations returned by Concraft based on Morfeusz2's analysis.\n",
    "        \n",
    "    Returns:\n",
    "        A list containing only the highest probability interpretations for every token.    \n",
    "    '''\n",
    "    \n",
    "    while(\"\" in dag_disamb):\n",
    "        dag_disamb.remove(\"\")\n",
    "    \n",
    "    best_inter = []\n",
    "    \n",
    "    for item in dag_disamb:\n",
    "        if item[0] == len(best_inter):\n",
    "            best_inter.append(item)\n",
    "        else:\n",
    "            if item[3] > best_inter[-1][3]:\n",
    "                best_inter[-1] = item\n",
    "        \n",
    "            \n",
    "    return best_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da080c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalAnnotations:\n",
    "    '''A class intended to process and store the tokens and their respective annotations from the original text.\n",
    "    \n",
    "    Attributes:\n",
    "        tokens (list[str]): A list of all the tokens (without annotation) in the original data. Every element of the list is a string.\n",
    "        gold_standard (list[str]): A list of all the original annotations (without tokens). Every element is a string.\n",
    "        sentences (list[str]): A list of the original sentences (either truly original, if available, or reconstructed).\n",
    "        sentences_tokenized (list[list[str]]): A list of lists representing tokenized, unannotated sentences.\n",
    "        gold_standard_tokenized (list[list[str]]): A list of lists representing the gold standard annotations per sentence.\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, filename: str, lowercase: bool = False, original: str = '', nested: bool = True):\n",
    "        '''The __init__ method of the class.\n",
    "        Constructs the token and tag lists.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): The name of the file that the text and the annotations are to be obtained from.\n",
    "            lowercase (bool): Determines whether the tokens should be lowercased or if original capitalization should be retained.\n",
    "            original (str): The name of the file containing the original unannotated sentences. If left empty or invalid,\n",
    "                reconstructed sentences will be used.\n",
    "            nested (bool): Determines whether the output list contains nested sublists.\n",
    "        '''\n",
    "        # opening the annotated text\n",
    "        text = []\n",
    "        with open(filename) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                text.append(line.strip())     \n",
    "        \n",
    "        # setting up attributes\n",
    "        self.tokens = []\n",
    "        self.gold_standard = []\n",
    "        \n",
    "        self.sentences = []\n",
    "        \n",
    "        self.sentences_tokenized = []\n",
    "        self.gold_standard_tokenized = []\n",
    "        \n",
    "        # reading in unannotated sentences, if applicable\n",
    "        if os.path.exists(original):\n",
    "            with open(original) as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    self.sentences.append(line.strip())\n",
    "        else:\n",
    "            original = ''\n",
    "        \n",
    "        # splitting tokens and annotation\n",
    "        for sentence in text:\n",
    "            # setting up temporary lists to store the values per sentence\n",
    "            temp_sent = []\n",
    "            temp_anns = []\n",
    "            temp_conc = []\n",
    "            temp_conc_ann = []\n",
    "            # setting up a string for potential concatenation of split elements\n",
    "            full_sent = ''\n",
    "            # setting up variables controlling the extent of concatenation / grouping together of elements\n",
    "            concatenate = False\n",
    "            concatenate_next = False\n",
    "            \n",
    "            sentence = sentence.strip()\n",
    "            sentence = sentence.split(\" \")\n",
    "            \n",
    "            for annotated_token in sentence:\n",
    "                \n",
    "                if concatenate_next == True:\n",
    "                    concatenate = False\n",
    "                    concatenate_next = False\n",
    "                \n",
    "                split_token = annotated_token.split('_')\n",
    "                \n",
    "                if split_token[0][0] == '[':\n",
    "                    split_token[0] = split_token[0][1:]\n",
    "                    concatenate = True\n",
    "                \n",
    "                if split_token[-1].endswith('AUX]'):\n",
    "                    split_token[-1] = split_token[-1][:-1]\n",
    "                    concatenate = False\n",
    "                \n",
    "                if split_token[-1][-1] == ']':\n",
    "                    split_token[-1] = split_token[-1][:-1]\n",
    "                    concatenate_next = True\n",
    "                    \n",
    "                if concatenate:\n",
    "                    full_sent += split_token[0]\n",
    "                    temp_conc.append(split_token[0])\n",
    "                    temp_conc_ann.append(split_token[1])\n",
    "                else:\n",
    "                    full_sent += (split_token[0] + ' ')\n",
    "                    if len(temp_conc) != 0:\n",
    "                        temp_sent.append(temp_conc)\n",
    "                        temp_anns.append(temp_conc_ann)\n",
    "                        temp_conc = []\n",
    "                        temp_conc_ann = []\n",
    "                \n",
    "                if not lowercase:\n",
    "                    self.tokens.append(split_token[0])\n",
    "                    if not concatenate:\n",
    "                        temp_sent.append(split_token[0])\n",
    "                        temp_anns.append(split_token[1])\n",
    "                else:\n",
    "                    self.tokens.append(split_token[0].lower())\n",
    "                    if not concatenate:\n",
    "                        temp_sent.append(split_token[0].lower())\n",
    "                        temp_anns.append(split_token[1])\n",
    "                        \n",
    "                self.gold_standard.append(split_token[1])\n",
    "            \n",
    "            if not lowercase:\n",
    "                temp_sent.append(split_token[0])\n",
    "                temp_anns.append(split_token[1])\n",
    "            else:\n",
    "                temp_sent.append(split_token[0].lower())\n",
    "                temp_anns.append(split_token[1])\n",
    "            self.sentences_tokenized.append(temp_sent)\n",
    "            self.gold_standard_tokenized.append(temp_anns)\n",
    "            \n",
    "            # reconstructing sentences if necessary\n",
    "            if original == '':\n",
    "                # removing unnecessary whitespace\n",
    "                full_sent = full_sent.replace(' .', '.')\n",
    "                full_sent = full_sent.replace(' ,', ',')\n",
    "                full_sent = full_sent.replace(' !', '!')\n",
    "                full_sent = full_sent.replace(' ?', '?')\n",
    "                full_sent = full_sent.replace(' :', ':')\n",
    "                full_sent = full_sent.replace('„ ', '„')\n",
    "                full_sent = full_sent.replace(' ”', '”')\n",
    "                full_sent = full_sent.replace('( ', '(')\n",
    "                full_sent = full_sent.replace(' )', ')')\n",
    "                full_sent = full_sent.strip()\n",
    "                self.sentences.append(full_sent)\n",
    "                \n",
    "        # removing the \"nested\" lists from tokenized sentences and their annotations that represent\n",
    "        # elements that should go together\n",
    "        if not nested:\n",
    "            for i, sent in enumerate(self.sentences_tokenized):\n",
    "                temp_sent = []\n",
    "                for token in sent:\n",
    "                    if isinstance(token, str):\n",
    "                        temp_sent.append(token)\n",
    "                    else:\n",
    "                        for element in token:\n",
    "                            temp_sent.append(element)\n",
    "                self.sentences_tokenized[i] = temp_sent\n",
    "                \n",
    "            for i, sent in enumerate(self.gold_standard_tokenized):\n",
    "                temp_ann = []\n",
    "                for token in sent:\n",
    "                    if isinstance(token, str):\n",
    "                        temp_ann.append(token)\n",
    "                    else:\n",
    "                        for element in token:\n",
    "                            temp_ann.append(element)\n",
    "                self.gold_standard_tokenized[i] = temp_ann\n",
    "                \n",
    "    def __len__(self):\n",
    "        '''The __len__ magic method of the class.\n",
    "            \n",
    "        Returns:\n",
    "            The length of self.tokens, which should be identical to the length of self.gold_standard.\n",
    "        '''\n",
    "        return len(self.tokens)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        '''The __getitem__ magic method of the class.\n",
    "            \n",
    "        Args:\n",
    "            index (int): The index signifying the desired element.\n",
    "            \n",
    "        Returns:\n",
    "            A string representing the combination of the original token and annotation.\n",
    "        '''\n",
    "        token = self.tokens[index]\n",
    "        annotation = self.gold_standard[index]\n",
    "        item = '_'.join([token, annotation])\n",
    "            \n",
    "        return item\n",
    "    \n",
    "    def frequencies(self):\n",
    "        '''A method of the class indended for displaying raw and relative frequencies of word classes in the annotation.\n",
    "        \n",
    "        Returns:\n",
    "            A dataframe representing the POS tag, raw frequency, relative frequency.\n",
    "        '''\n",
    "        freqs = []\n",
    "        for item in list(set(self.gold_standard)):\n",
    "            raw = self.gold_standard.count(item)\n",
    "            relative = raw / len(self.gold_standard)\n",
    "            \n",
    "            freqs.append([item, raw, relative])\n",
    "            \n",
    "        freq_pd = pd.DataFrame(freqs, columns=['POS', 'raw', 'relative']).sort_values('relative', ascending=False).set_index('POS')\n",
    "            \n",
    "        return freq_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "179af228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConlluWord():\n",
    "    '''A class intended to represent a line in a conll file (1 word, respectively).\n",
    "    \n",
    "    Attributes:\n",
    "        ID (str): The index number of the word.\n",
    "        FORM (str): The word as it appears in the sentence.\n",
    "        LEMMA (str): The lemma of the word.\n",
    "        UPOS (str): Universal Dependencies' universal part of speach tag.\n",
    "        XPOS (str): Language-specific part of speech tag.\n",
    "        FEATS (str): Features. Empty by default.\n",
    "        HEAD (str): Signifies which word is the head of this one. Empty by default.\n",
    "        DEPREL (str): Represents the dependency relations. Empty by default.\n",
    "        DEPS (str): The dependencies of this word. Empty by default.\n",
    "        MISC (str): Miscellaneous information. Empty by default.\n",
    "        \n",
    "    '''\n",
    "    def __init__(\n",
    "        self, ID: str, FORM: str, LEMMA: str, UPOS: str, XPOS: str, FEATS: str = '_', \n",
    "        HEAD: str = '_', DEPREL: str = '_', DEPS: str = '_', MISC: str = '_'\n",
    "    ):\n",
    "        '''The __init__ method of the class. Assigns the values to the ConLLu tags.\n",
    "        \n",
    "        Args:\n",
    "            ID (str): The index number of the word.\n",
    "            FORM (str): The word as it appears in the sentence.\n",
    "            LEMMA (str): The lemma of the word.\n",
    "            UPOS (str): Universal Dependencies' universal part of speach tag.\n",
    "            XPOS (str): Language-specific part of speech tag.\n",
    "            FEATS (str): Features. Empty by default.\n",
    "            HEAD (str): Signifies which word is the head of this one. Empty by default.\n",
    "            DEPREL (str): Represents the dependency relations. Empty by default.\n",
    "            DEPS (str): The dependencies of this word. Empty by default.\n",
    "            MISC (str): Miscellaneous information. Empty by default.\n",
    "        '''\n",
    "        self.ID = ID\n",
    "        self.FORM = FORM\n",
    "        self.LEMMA = LEMMA\n",
    "        self.UPOS = UPOS\n",
    "        self.XPOS = XPOS\n",
    "        self.FEATS = FEATS\n",
    "        self.HEAD = HEAD\n",
    "        self.DEPREL = DEPREL\n",
    "        self.DEPS = DEPS\n",
    "        self.MISC = MISC\n",
    "        \n",
    "    def return_line(self):\n",
    "        '''A method of the class that returns all the tags in the form of a tab-separated string, as per the ConLLu format.\n",
    "        '''\n",
    "        elements = [\n",
    "            self.ID, self.FORM, self.LEMMA, self.UPOS, self.XPOS, self.FEATS, self.HEAD, \n",
    "            self.DEPREL, self.DEPS, self.MISC]\n",
    "        line = \"\\t\".join(elements)\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2a99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConlluSentence():\n",
    "    '''A class intended to represent a sentence entry in a conll file.\n",
    "    \n",
    "    Attributes:\n",
    "        sent_id (str): The ID of the sentence.\n",
    "        sent (str): The sentence itself.\n",
    "        words (list[ConlluWord]): A list of ConlluWord objects representing constituent words and their annotation.\n",
    "    '''\n",
    "    def __init__(self, sent_id: str, sent: str, words: list):\n",
    "        '''The __init__ method of the class. Assigns the arguments to internal attributes.\n",
    "        Args:\n",
    "            sent_id (str): The ID of the sentence.\n",
    "            sent (str): The sentence itself.\n",
    "            words (list[ConlluWord]): A list of ConlluWord objects representing constituent words and their annotation.\n",
    "        ''' \n",
    "        self.sent_id = sent_id\n",
    "        self.sent = sent\n",
    "        self.words = words\n",
    "        \n",
    "    def return_sent(self):\n",
    "        '''A method of the class that returns a sentence entry.\n",
    "        '''\n",
    "        whole_sent = '\\n'.join([self.sent_id, self.sent] + [x.return_line() for x in self.words])\n",
    "        return whole_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "909c770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConlluFormatter():\n",
    "    '''A class intended to create a representation of the input text in ConLLu format using pre-annotated tags as well as\n",
    "    annotation from Morfeusz and Concraft.\n",
    "    \n",
    "    Attributes:\n",
    "        all_conll_sents (list[list]): A list of lists representing the sentences with their annotation stored.        \n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        sents: list, \n",
    "        tokenized: list, \n",
    "        anns: list, \n",
    "        morfeusz=morfeusz, \n",
    "        concraft=concraft\n",
    "    ):\n",
    "        '''The __init__ method of the class.\n",
    "        Creates a list for every sentence in the input that contains annotations for every word using the ConlluWord class.\n",
    "        \n",
    "        Args:\n",
    "            sents (list[str]): A list of sentences as strings.\n",
    "            tokenized (list[list[str]]): A list of tokenized sentences (in the same order as in sents).\n",
    "            anns (list[list[str]]): A list of annotations corresponding to the words in tokenized.\n",
    "            morfeusz (Morfeusz): a Morfeusz object that will be used for morphological analysis of the sentences.\n",
    "            concraft (Concraft): a Concraft object that will be used for morphological disambiguation and annotation.\n",
    "        '''\n",
    "        # sanity check\n",
    "        if len(sents)!=len(tokenized) or len(sents)!=len(anns):\n",
    "            print('Input lists not of equal length.')\n",
    "            return\n",
    "        \n",
    "        self.all_conll_sents = []\n",
    "        \n",
    "        for i, sent in enumerate(sents):\n",
    "            conll_sent = []\n",
    "            \n",
    "            dag = morfeusz.analyse(sent)\n",
    "            dag_disamb = concraft.disamb(dag)\n",
    "            best_inter = best_interpretation(dag_disamb)\n",
    "            \n",
    "            ann = anns[i]\n",
    "            tokens = tokenized[i]\n",
    "            \n",
    "            offset = 0\n",
    "            \n",
    "            for j, inter in enumerate(best_inter):\n",
    "                idx = str(j + 1)\n",
    "                form = inter[2][0]\n",
    "                if form == \"ś\":\n",
    "                    if best_inter[j-1][2][0].endswith('ś'):\n",
    "                        offset += -1\n",
    "                        continue\n",
    "                lemma = inter[2][1].split(':')[0]\n",
    "                try:\n",
    "                    if tokens[j+offset] == form:\n",
    "                        upos = ann[j+offset]      \n",
    "                    elif tokens[j+offset] + tokens[j+offset+1] == form:\n",
    "                        upos = ann[j+offset]\n",
    "                        offset += 1\n",
    "                    else:    \n",
    "                        upos = '_'\n",
    "                except IndexError:\n",
    "                    continue\n",
    "       \n",
    "                xpos = inter[2][2]\n",
    "                \n",
    "                word = ConlluWord(idx, form, lemma, upos, xpos)\n",
    "                conll_sent.append(word)\n",
    "            \n",
    "            full_sent = ConlluSentence('# sent_id = ' + str(i+1), '# text = ' + sent, conll_sent)\n",
    "            self.all_conll_sents.append(full_sent)\n",
    "            \n",
    "    def print_conll(self):\n",
    "        '''A method of the class indended for printing out all of the annotation in the ConLLu format.\n",
    "        '''\n",
    "        for sentence in self.all_conll_sents:\n",
    "            print(sentence.return_sent())\n",
    "            print('\\n')\n",
    "        \n",
    "    def write_conll_2_file(self, filename: str):\n",
    "        '''A method of the class indended for displaying saving all of the annotation in the ConLLu format.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): The name of the file the data should be saved to.\n",
    "        '''      \n",
    "        with open(filename, 'w') as f:\n",
    "            for sentence in self.all_conll_sents:\n",
    "                f.write(sentence.return_sent() + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aacf5d",
   "metadata": {},
   "source": [
    "### TESTING AROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c35496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dag = morfeusz.analyse(original_text.sentences[312])\n",
    "#dag_disamb = concraft.disamb(dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37eebc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_interpretation(dag_disamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6d9c5",
   "metadata": {},
   "source": [
    "### EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f90ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = OriginalAnnotations(file, lowercase=False, nested=False)\n",
    "#print(original_text.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bf735c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = ConlluFormatter(\n",
    "    original_text.sentences, \n",
    "    original_text.sentences_tokenized, \n",
    "    original_text.gold_standard_tokenized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33b11557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatter.print_conll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe1f75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter.write_conll_2_file('../data/memoirs_10k.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO ADD COMPOUND PHRASE HANDLING!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
