{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b446bea7",
   "metadata": {},
   "source": [
    "# GATE CLOUD UD TAGGER EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3677803",
   "metadata": {},
   "source": [
    "### IMPORTS, VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee5c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import math\n",
    "import sklearn.metrics\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2fc9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://cloud.gate.ac.uk/shopfront/displayItem/tagger-pos-pl-maxent1 - \"A POS tagger for pl / Polish using the Universal \n",
    "# Dependencies POS tagset.\n",
    "# instructions on how to use the API from https://cloud.gate.ac.uk/info/help/online-api.html\n",
    "\n",
    "URL = 'https://cloud-api.gate.ac.uk/process/tagger-pos-pl-maxent1'\n",
    "PARAMS = {\n",
    "    'Content-Type': 'text/plain',\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# because I am reusing code from an earlier project, it will be easier for me to import the data from\n",
    "file = '../data/memoirs_10k_corrected.conllu'\n",
    "test_file = '../data/ud-treebanks/UD_Polish-PDB/pl_pdb-ud-test.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5ed076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from preproc_bert import remove_ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd320d8",
   "metadata": {},
   "source": [
    "### FUNCTIONS AND CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618fc832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggerAnnotations:\n",
    "    '''A class intended for retrieving, processing, and storing the annotations from an online tagger.\n",
    "    \n",
    "    Attributes:\n",
    "        processed_annotations (list[list]): A list of lists, every element of which represents a number of values returned by the\n",
    "            tagger. The values include the token, the POS tag, the confidence thereof, a list of possible targets and a list\n",
    "            of those confidences.\n",
    "        only_annotations (list[str]): A list containing only the POS tags obtained from the tagger.  \n",
    "        data (list[str]): The original input data.\n",
    "    '''\n",
    "    def __init__(self, url: str, headers: dict, data: list):\n",
    "        '''The __init__ method of the class.\n",
    "        Retrieves the tagger annotations (in batches, if needed), constructs processed_annotations and only_annotations using\n",
    "        the __retrieve_anns method.\n",
    "        \n",
    "        Args:\n",
    "            url (str): The URL to which the query should be made.\n",
    "            headers (dict): The parameters that the query requires.\n",
    "            data (list[list[str]]): The original, unannotated data represented as a list of lists of strings.        \n",
    "        '''\n",
    "        self.processed_annotations = []\n",
    "        self.only_annotations = []\n",
    "        self.data = data      \n",
    "    \n",
    "        for i, sent in enumerate(tqdm(data, desc='Loading sentences...')):\n",
    "            sent = ' '.join(sent)\n",
    "\n",
    "            r = requests.post(url=url, data=sent.encode('utf-8'), headers=headers) \n",
    "            annotations = r.json()\n",
    "\n",
    "            for entry in annotations['entities']['Token']:\n",
    "                self.__retrieve_anns(entry)\n",
    "                \n",
    "        self.__sanity_check()      \n",
    "\n",
    "    def print_annotations(self):\n",
    "        '''A method of the class which prints out all words with their annotation and the confidence thereof.\n",
    "        '''\n",
    "        for entry in self.processed_annotations:\n",
    "            print(entry[0] + ' | ' + entry[1] + ' | ' + str(entry[2]))\n",
    "            \n",
    "    def __retrieve_anns(self, entry: dict):\n",
    "        '''A method of the class which reads the data returned by the tagger and stores the relevant elements in appropriate\n",
    "        lists.\n",
    "        \n",
    "        Args:\n",
    "            entry (dict): An entry returned by the tagger.\n",
    "        '''\n",
    "        word = entry['string']\n",
    "        pos = entry['upos']\n",
    "        confidence = entry['LF_confidence']\n",
    "        target_list = entry['LF_target_list']\n",
    "        confidence_list = entry['LF_confidence_list']\n",
    "\n",
    "        annotation = [word, pos, confidence, target_list, confidence_list]\n",
    "        self.processed_annotations.append(annotation)\n",
    "        self.only_annotations.append(pos)\n",
    "        \n",
    "    def __sanity_check(self):\n",
    "        '''A method of the class that allows for the fixing of length mismatched between the input and the output due\n",
    "        to the tagger tokenizing the text differently than the annotator. The annotation lists only preserve the annotation\n",
    "        corresponding to the last part of the mistokenized item, as that one most commonly defines the word class. This works \n",
    "        up to mistokenizations of the length of 3.\n",
    "        '''\n",
    "        for i, token in enumerate([x for sentence in self.data for x in sentence]):\n",
    "            if self.processed_annotations[i][0] != token:\n",
    "                if self.processed_annotations[i][0] + self.processed_annotations[i+1][0] == token:\n",
    "                    del self.processed_annotations[i]\n",
    "                    del self.only_annotations[i]\n",
    "                elif self.processed_annotations[i][0] + self.processed_annotations[i+1][0] + self.processed_annotations[i+2][0] == token:\n",
    "                    del self.processed_annotations[i]\n",
    "                    del self.only_annotations[i]\n",
    "                    del self.processed_annotations[i+1]\n",
    "                    del self.only_annotations[i+1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''The __len__ magic method of the class.\n",
    "            \n",
    "        Returns:\n",
    "            The length of self.only_annotations, which should be identical to the length of self.processed_annotations.\n",
    "        '''\n",
    "        return len(self.only_annotations)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        '''The __getitem__ magic method of the class.\n",
    "            \n",
    "        Args:\n",
    "            index (int): The index signifying the desired element.\n",
    "            \n",
    "        Returns:\n",
    "            A string representing the combination of the original token and the tagger annotation.\n",
    "        '''\n",
    "        full_ann = self.processed_annotations[index]\n",
    "        token = full_ann[0]\n",
    "        annotation = full_ann[1]\n",
    "        item = '_'.join([token, annotation])\n",
    "            \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43debfe0",
   "metadata": {},
   "source": [
    "### EXECUTION - STANDARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "391d2041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_tokens_features = extract_conllu_data(test_file, 'upos', sentences=False, combined=True, fulltext=False)\n",
    "#test_tokens, test_features = make_tagger_friendly(test_tokens_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc9fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(test_tokens))\n",
    "# due to limitations of the tagger, giving it more than 30k tokens is impossible within one notebook - even across separate\n",
    "# instances of the class object; therefore, only a chunk of the test data can be fed to the tagger \n",
    "#test_tokens = test_tokens[:20000]\n",
    "#test_features = test_features[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "244939d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_tagger_anns = TaggerAnnotations(URL, PARAMS, test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f6dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_measures([x for sentence in test_features for x in sentence], tagger_anns.only_annotations, details=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaf0db0",
   "metadata": {},
   "source": [
    "### EXECUTION - PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49791112",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_features = extract_conllu_data(file, 'upos', sentences=True, combined=True, fulltext=False)\n",
    "tokens, features = make_tagger_friendly(tokens_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9093b427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sentences...:   8%|████▊                                                       | 29/360 [00:13<02:30,  2.20it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'entities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_897/265061511.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagger_anns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTaggerAnnotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPARAMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_897/1143030849.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url, headers, data)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__retrieve_anns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entities'"
     ]
    }
   ],
   "source": [
    "tagger_anns = TaggerAnnotations(URL, PARAMS, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab724c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_measures([x for sentence in features for x in sentence], tagger_anns.only_annotations, details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e7639",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = get_comparison(\n",
    "    [x for sentence in features for x in sentence], \n",
    "    tagger_anns.only_annotations, \n",
    "    [x for sentence in tokens for x in sentence], \n",
    "    confidence=[x[2] for x in tagger_anns.processed_annotations])\n",
    "    \n",
    "comparison.to_excel('../data/mistakes/UD_UPOS_mistakes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab804ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff139fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
