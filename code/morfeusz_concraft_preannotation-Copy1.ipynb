{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0206f6f4",
   "metadata": {},
   "source": [
    "# MORFEUSZ & CONCRAFT - PREANNOTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48b099",
   "metadata": {},
   "source": [
    "### IMPORTS, VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c688d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/kawu/concraft-pl\n",
    "# https://github.com/kawu/concraft-pl/tree/master/bindings/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab283a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "sys.path.append('../concraft-pl/bindings/python/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5247dac3",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'concraft-pl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmorfeusz2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Morfeusz\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcraft_pl2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Concraft, Server\n\u001b[0;32m----> 4\u001b[0m server \u001b[38;5;241m=\u001b[39m \u001b[43mServer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../concraft-pl/model-SGJP.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/memoirs_annotated_10k.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m original_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/memoirs_10k.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/mnt/c/Users/Maria/LW_share/ma-thesis-swe/code/../concraft-pl/bindings/python/concraft_pl2.py:96\u001b[0m, in \u001b[0;36mServer.__init__\u001b[0;34m(self, model_path, concraft_path, port, core_num, allocation_size)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03mStart a Concraft-pl server instance in the background.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    Allocation area size (in MBs) of the garbage collector\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport \u001b[38;5;241m=\u001b[39m port\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcraft_server \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconcraft_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mserver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--port=\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-i\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m+RTS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-N\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore_num\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-A\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallocation_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# print(u\"Concraft model \" + model_path + u\" loading...\")\u001b[39;00m\n\u001b[1;32m    101\u001b[0m loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m    855\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    856\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1704\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1703\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'concraft-pl'"
     ]
    }
   ],
   "source": [
    "from morfeusz2 import Morfeusz\n",
    "from concraft_pl2 import Concraft, Server\n",
    "\n",
    "server = Server(model_path='../concraft-pl/model-SGJP.gz')\n",
    "file = '../data/memoirs_annotated_10k.txt'\n",
    "original_file = '../data/memoirs_10k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0012aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "morfeusz = Morfeusz(expand_tags=True)\n",
    "concraft = Concraft()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4643c6d",
   "metadata": {},
   "source": [
    "### FUNCTIONS AND CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_interpretation(dag_disamb: list):\n",
    "    '''A function that allows for the selection of only the best possible morphosyntactic interpretation of a sentence as\n",
    "    returned by Morfeusz2 + Concraft.\n",
    "    \n",
    "    Args:\n",
    "        dag_disamb (list): A list of possible interpretations returned by Concraft based on Morfeusz2's analysis.\n",
    "        \n",
    "    Returns:\n",
    "        A list containing only the highest probability interpretations for every token.    \n",
    "    '''\n",
    "    \n",
    "    while(\"\" in dag_disamb):\n",
    "        dag_disamb.remove(\"\")\n",
    "    \n",
    "    best_inter = []\n",
    "    \n",
    "    for item in dag_disamb:\n",
    "        if item[0] == len(best_inter):\n",
    "            best_inter.append(item)\n",
    "        else:\n",
    "            if item[3] > best_inter[-1][3]:\n",
    "                best_inter[-1] = item\n",
    "        \n",
    "            \n",
    "    return best_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da080c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalAnnotations:\n",
    "    '''A class intended to process and store the tokens and their respective annotations from the original text.\n",
    "    \n",
    "    Attributes:\n",
    "        tokens (list[str]): A list of all the tokens (without annotation) in the original data. Every element of the list is a string.\n",
    "        gold_standard (list[str]): A list of all the original annotations (without tokens). Every element is a string.\n",
    "        sentences (list[str]): A list of the original sentences (either truly original, if available, or reconstructed).\n",
    "        sentences_tokenized (list[list[str]]): A list of lists representing tokenized, unannotated sentences.\n",
    "        gold_standard_tokenized (list[list[str]]): A list of lists representing the gold standard annotations per sentence.\n",
    "        simple_sentences_tokenized (list[list[str]]): A list of lists representing the tokenized, unannotated sentences, with no regard\n",
    "        as to whether the tokens were written together or not.\n",
    "        simple_gold_standard_tokenized (list[list[str]]): A list of lists representing the gold standard annotations per sentence, \n",
    "        with no regard as to whether the tokens were written together or not.\n",
    "    '''\n",
    "    def __init__(self, filename: str, lowercase: bool = False, original: str = ''):\n",
    "        '''The __init__ method of the class.\n",
    "        Constructs the token and tag lists.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): The name of the file that the text and the annotations are to be obtained from.\n",
    "            lowercase (bool): Determines whether the tokens should be lowercased or if original capitalization should be retained.\n",
    "            original (str): The name of the file containing the original unannotated sentences. If left empty or invalid,\n",
    "                reconstructed sentences will be used.\n",
    "            nested (bool): Determines whether the output list contains nested sublists.\n",
    "        '''\n",
    "        # opening the annotated text\n",
    "        text = []\n",
    "        with open(filename) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                text.append(line.strip())     \n",
    "        \n",
    "        # setting up attributes\n",
    "        self.tokens = []\n",
    "        self.gold_standard = []\n",
    "        \n",
    "        self.sentences = []\n",
    "        \n",
    "        self.sentences_tokenized = []\n",
    "        self.gold_standard_tokenized = []\n",
    "        \n",
    "        self.simple_sentences_tokenized = []\n",
    "        self.simple_gold_standard_tokenized = []\n",
    "        \n",
    "        # reading in unannotated sentences, if applicable\n",
    "        if os.path.exists(original):\n",
    "            with open(original) as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    self.sentences.append(line.strip())\n",
    "        else:\n",
    "            original = ''\n",
    "        \n",
    "        # splitting tokens and annotation\n",
    "        for sentence in text:\n",
    "            # setting up temporary lists to store the values per sentence\n",
    "            temp_sent = []\n",
    "            temp_anns = []\n",
    "            temp_conc = []\n",
    "            temp_conc_ann = []\n",
    "            # setting up a string for potential concatenation of split elements\n",
    "            full_sent = ''\n",
    "            # setting up variables controlling the extent of concatenation / grouping together of elements\n",
    "            concatenate = False\n",
    "            \n",
    "            sentence = sentence.strip()\n",
    "            sentence = sentence.split(\" \")\n",
    "            \n",
    "            for annotated_token in sentence:\n",
    "                \n",
    "                split_token = annotated_token.split('_')\n",
    "                \n",
    "                if split_token[0][0] == '[':\n",
    "                    split_token[0] = split_token[0][1:]\n",
    "                    concatenate = True\n",
    "                \n",
    "                if split_token[-1].endswith(']'):\n",
    "                    split_token[-1] = split_token[-1][:-1]\n",
    "                    concatenate = False\n",
    "                    \n",
    "                if concatenate:\n",
    "                    full_sent += split_token[0]\n",
    "                    temp_conc.append(split_token[0])\n",
    "                    temp_conc_ann.append(split_token[1])\n",
    "                else:\n",
    "                    full_sent += (split_token[0] + ' ')\n",
    "                    if len(temp_conc) != 0:\n",
    "                        temp_sent.append(temp_conc)\n",
    "                        temp_anns.append(temp_conc_ann)\n",
    "                        temp_conc = []\n",
    "                        temp_conc_ann = []\n",
    "                \n",
    "                if not lowercase:\n",
    "                    self.tokens.append(split_token[0])\n",
    "                    if not concatenate:\n",
    "                        temp_sent.append(split_token[0])\n",
    "                        temp_anns.append(split_token[1])\n",
    "                else:\n",
    "                    self.tokens.append(split_token[0].lower())\n",
    "                    if not concatenate:\n",
    "                        temp_sent.append(split_token[0].lower())\n",
    "                        temp_anns.append(split_token[1])\n",
    "                        \n",
    "                self.gold_standard.append(split_token[1])\n",
    "            \n",
    "            if not lowercase:\n",
    "                temp_sent.append(split_token[0])\n",
    "                temp_anns.append(split_token[1])\n",
    "            else:\n",
    "                temp_sent.append(split_token[0].lower())\n",
    "                temp_anns.append(split_token[1])\n",
    "            self.sentences_tokenized.append(temp_sent)\n",
    "            self.gold_standard_tokenized.append(temp_anns)\n",
    "            \n",
    "            # reconstructing sentences if necessary\n",
    "            if original == '':\n",
    "                # removing unnecessary whitespace\n",
    "                full_sent = full_sent.replace(' .', '.')\n",
    "                full_sent = full_sent.replace(' ,', ',')\n",
    "                full_sent = full_sent.replace(' !', '!')\n",
    "                full_sent = full_sent.replace(' ?', '?')\n",
    "                full_sent = full_sent.replace(' :', ':')\n",
    "                full_sent = full_sent.replace('„ ', '„')\n",
    "                full_sent = full_sent.replace(' ”', '”')\n",
    "                full_sent = full_sent.replace('( ', '(')\n",
    "                full_sent = full_sent.replace(' )', ')')\n",
    "                full_sent = full_sent.strip()\n",
    "                self.sentences.append(full_sent)\n",
    "                \n",
    "        # removing the \"nested\" lists from tokenized sentences and their annotations that represent\n",
    "        # elements that should go together\n",
    "        for i, sent in enumerate(self.sentences_tokenized):\n",
    "            temp_sent = []\n",
    "            for token in sent:\n",
    "                if isinstance(token, str):\n",
    "                    temp_sent.append(token)\n",
    "                else:\n",
    "                    for element in token:\n",
    "                        temp_sent.append(element)\n",
    "            self.simple_sentences_tokenized.append(temp_sent)\n",
    "                \n",
    "        for i, sent in enumerate(self.gold_standard_tokenized):\n",
    "            temp_ann = []\n",
    "            for token in sent:\n",
    "                if isinstance(token, str):\n",
    "                    temp_ann.append(token)\n",
    "                else:\n",
    "                    for element in token:\n",
    "                        temp_ann.append(element)\n",
    "            self.simple_gold_standard_tokenized.append(temp_ann)\n",
    "                \n",
    "    def __len__(self):\n",
    "        '''The __len__ magic method of the class.\n",
    "            \n",
    "        Returns:\n",
    "            The length of self.tokens, which should be identical to the length of self.gold_standard.\n",
    "        '''\n",
    "        return len(self.tokens)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        '''The __getitem__ magic method of the class.\n",
    "            \n",
    "        Args:\n",
    "            index (int): The index signifying the desired element.\n",
    "            \n",
    "        Returns:\n",
    "            A string representing the combination of the original token and annotation.\n",
    "        '''\n",
    "        token = self.tokens[index]\n",
    "        annotation = self.gold_standard[index]\n",
    "        item = '_'.join([token, annotation])\n",
    "            \n",
    "        return item\n",
    "    \n",
    "    def frequencies(self):\n",
    "        '''A method of the class indended for displaying raw and relative frequencies of word classes in the annotation.\n",
    "        \n",
    "        Returns:\n",
    "            A dataframe representing the POS tag, raw frequency, relative frequency.\n",
    "        '''\n",
    "        freqs = []\n",
    "        for item in list(set(self.gold_standard)):\n",
    "            raw = self.gold_standard.count(item)\n",
    "            relative = raw / len(self.gold_standard)\n",
    "            \n",
    "            freqs.append([item, raw, relative])\n",
    "            \n",
    "        freq_pd = pd.DataFrame(freqs, columns=['POS', 'raw', 'relative']).sort_values('relative', ascending=False).set_index('POS')\n",
    "            \n",
    "        return freq_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179af228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConlluWord():\n",
    "    '''A class intended to represent a line in a conll file (1 word, respectively).\n",
    "    \n",
    "    Attributes:\n",
    "        ID (str): The index number of the word.\n",
    "        FORM (str): The word as it appears in the sentence.\n",
    "        LEMMA (str): The lemma of the word.\n",
    "        UPOS (str): Universal Dependencies' universal part of speach tag.\n",
    "        XPOS (str): Language-specific part of speech tag.\n",
    "        FEATS (str): Features. Empty by default.\n",
    "        HEAD (str): Signifies which word is the head of this one. Empty by default.\n",
    "        DEPREL (str): Represents the dependency relations. Empty by default.\n",
    "        DEPS (str): The dependencies of this word. Empty by default.\n",
    "        MISC (str): Miscellaneous information. Empty by default.\n",
    "        \n",
    "    '''\n",
    "    def __init__(\n",
    "        self, ID: str, FORM: str, LEMMA: str, UPOS: str, XPOS: str, FEATS: str = '_', \n",
    "        HEAD: str = '_', DEPREL: str = '_', DEPS: str = '_', MISC: str = '_'\n",
    "    ):\n",
    "        '''The __init__ method of the class. Assigns the values to the ConLLu tags.\n",
    "        \n",
    "        Args:\n",
    "            ID (str): The index number of the word.\n",
    "            FORM (str): The word as it appears in the sentence.\n",
    "            LEMMA (str): The lemma of the word.\n",
    "            UPOS (str): Universal Dependencies' universal part of speach tag.\n",
    "            XPOS (str): Language-specific part of speech tag.\n",
    "            FEATS (str): Features. Empty by default.\n",
    "            HEAD (str): Signifies which word is the head of this one. Empty by default.\n",
    "            DEPREL (str): Represents the dependency relations. Empty by default.\n",
    "            DEPS (str): The dependencies of this word. Empty by default.\n",
    "            MISC (str): Miscellaneous information. Empty by default.\n",
    "        '''\n",
    "        self.ID = ID\n",
    "        self.FORM = FORM\n",
    "        self.LEMMA = LEMMA\n",
    "        self.UPOS = UPOS\n",
    "        self.XPOS = XPOS\n",
    "        self.FEATS = FEATS\n",
    "        self.HEAD = HEAD\n",
    "        self.DEPREL = DEPREL\n",
    "        self.DEPS = DEPS\n",
    "        self.MISC = MISC\n",
    "        \n",
    "    def return_line(self):\n",
    "        '''A method of the class that returns all the tags in the form of a tab-separated string, as per the ConLLu format.\n",
    "        '''\n",
    "        elements = [\n",
    "            self.ID, self.FORM, self.LEMMA, self.UPOS, self.XPOS, self.FEATS, self.HEAD, \n",
    "            self.DEPREL, self.DEPS, self.MISC]\n",
    "        line = \"\\t\".join(elements)\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConlluSentence():\n",
    "    '''A class intended to represent a sentence entry in a conll file.\n",
    "    \n",
    "    Attributes:\n",
    "        sent_id (str): The ID of the sentence.\n",
    "        sent (str): The sentence itself.\n",
    "        words (list[ConlluWord]): A list of ConlluWord objects representing constituent words and their annotation.\n",
    "    '''\n",
    "    def __init__(self, sent_id: str, sent: str, words: list):\n",
    "        '''The __init__ method of the class. Assigns the arguments to internal attributes.\n",
    "        Args:\n",
    "            sent_id (str): The ID of the sentence.\n",
    "            sent (str): The sentence itself.\n",
    "            words (list[ConlluWord]): A list of ConlluWord objects representing constituent words and their annotation.\n",
    "        ''' \n",
    "        self.sent_id = sent_id\n",
    "        self.sent = sent\n",
    "        self.words = words\n",
    "        \n",
    "    def return_sent(self):\n",
    "        '''A method of the class that returns a sentence entry.\n",
    "        '''\n",
    "        whole_sent = '\\n'.join([self.sent_id, self.sent] + [x.return_line() for x in self.words])\n",
    "        return whole_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConlluFormatter():\n",
    "    '''A class intended to create a representation of the input text in ConLLu format using pre-annotated tags as well as\n",
    "    annotation from Morfeusz and Concraft.\n",
    "    \n",
    "    Attributes:\n",
    "        all_conll_sents (list[list]): A list of lists representing the sentences with their annotation stored.        \n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        annotations: OriginalAnnotations, \n",
    "        morfeusz=morfeusz, \n",
    "        concraft=concraft\n",
    "    ):\n",
    "        '''The __init__ method of the class.\n",
    "        Creates a list for every sentence in the input that contains annotations for every word using the ConlluWord class.\n",
    "        \n",
    "        Args:\n",
    "            sents (list[str]): A list of sentences as strings.\n",
    "            tokenized (list[list[str]]): A list of tokenized sentences (in the same order as in sents).\n",
    "            anns (list[list[str]]): A list of annotations corresponding to the words in tokenized.\n",
    "            morfeusz (Morfeusz): a Morfeusz object that will be used for morphological analysis of the sentences.\n",
    "            concraft (Concraft): a Concraft object that will be used for morphological disambiguation and annotation.\n",
    "        '''\n",
    "        self.all_conll_sents = []\n",
    "        \n",
    "        # retrieving the data sentence by sentence\n",
    "        for i, sent in enumerate(annotations.sentences):\n",
    "            conll_sent = []\n",
    "            # getting the Morfeusz + Concraft info\n",
    "            dag = morfeusz.analyse(sent)\n",
    "            dag_disamb = concraft.disamb(dag)\n",
    "            best_inter = best_interpretation(dag_disamb)\n",
    "            # retrieving the manual annotations as well as tokens corresponding to the annotation\n",
    "            ann = annotations.simple_gold_standard_tokenized[i]\n",
    "            tokens = annotations.simple_sentences_tokenized[i]\n",
    "            # setting up the offset that will be used for situations where tokens that were split in manual annotation were\n",
    "            # not split in the machine one\n",
    "            offset = 0\n",
    "            \n",
    "            # retrieving the data word by word\n",
    "            for j, inter in enumerate(best_inter):\n",
    "                # defining the index, retrieving the word as detected by Morfeusz\n",
    "                idx = str(j + 1)\n",
    "                form = inter[2][0]\n",
    "                # excluding mistakenly detected ś tokens (that were not even split from the preceding word)    \n",
    "                if form == \"ś\":\n",
    "                    if best_inter[j-1][2][0].endswith('ś'):\n",
    "                        offset += -1\n",
    "                        continue\n",
    "                # retrieving the lemma\n",
    "                if len(inter[2][1]) > 1:\n",
    "                    lemma = inter[2][1].split(':')[0]\n",
    "                else:  # for when the lemma is just ':'\n",
    "                    lemma = inter[2][1]\n",
    "                # retrieving the UPOS tag for the word from the manual annotation, updating the offset accordingly\n",
    "                try:\n",
    "                    if tokens[j+offset] == form:\n",
    "                        upos = ann[j+offset]      \n",
    "                    elif tokens[j+offset] + tokens[j+offset+1] == form:\n",
    "                        upos = ann[j+offset]\n",
    "                        offset += 1\n",
    "                    elif tokens[j+offset] + tokens[j+offset+1] + tokens[j+offset+2] == form:\n",
    "                        upos = ann[j+offset]\n",
    "                        offset += 2\n",
    "                    else:    \n",
    "                        upos = '_'\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                \n",
    "                # retrieving the XPOS tag\n",
    "                xpos = inter[2][2]\n",
    "                \n",
    "                # lowercasing the lemmas to match the UD standard\n",
    "                if upos != 'PROPN':\n",
    "                    lemma = lemma.lower()\n",
    "                \n",
    "                # creating a ConlluWord object to store the retrieved information, appending it to a temporary sentence list\n",
    "                word = ConlluWord(idx, form, lemma, upos, xpos)\n",
    "                conll_sent.append(word)\n",
    "            \n",
    "            # handling of compounded elements (only the ones marked with 'aglt' in XPOS are displayed this way by UD)\n",
    "            tracker = []\n",
    "            for j, word in enumerate(conll_sent):\n",
    "                if word.XPOS.startswith('aglt') and word.UPOS == 'AUX':\n",
    "                    if conll_sent[j+1].XPOS.startswith('aglt') and word.UPOS == 'AUX':\n",
    "                        tracker.append(\n",
    "                            (j-1, \n",
    "                             str(j)+'-'+str(j+2), \n",
    "                             conll_sent[j-1].FORM+word.FORM+conll_sent[j+1].FORM)\n",
    "                        ) \n",
    "                    else:  # only 2 words connected\n",
    "                        tracker.append((j-1, str(j)+'-'+str(j+1), conll_sent[j-1].FORM+word.FORM))\n",
    "            # adding the additional entries\n",
    "            for j, entry in reversed(list(enumerate(tracker))):\n",
    "                word = ConlluWord(entry[1], entry[2], '_', '_', '_')\n",
    "                conll_sent.insert(entry[0], word)\n",
    "            \n",
    "            # creating a ConlluSentence object, appending it to the internal list of all sentences\n",
    "            full_sent = ConlluSentence('# sent_id = ' + str(i+1), '# text = ' + sent, conll_sent)\n",
    "            self.all_conll_sents.append(full_sent)\n",
    "            \n",
    "    def __len__(self):\n",
    "        '''A method of the class that returns the length of the internal storage of ConLLu sentences.\n",
    "        '''\n",
    "        return len(self.all_conll_sents)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        '''A method of the class that returns the transformed sentence at a given index.\n",
    "        \n",
    "        Args:\n",
    "            index (int): The index of the desired element.\n",
    "        '''\n",
    "        return self.all_conll_sents[index]\n",
    "    \n",
    "    def print_item(self, index: int):\n",
    "        '''A method of the class that prints out the sentence at a given index.\n",
    "        \n",
    "        Args:\n",
    "            index (int): The index of the desired element.\n",
    "        '''\n",
    "        print(self.all_conll_sents[index].return_sent())\n",
    "    \n",
    "    def print_conllu(self):\n",
    "        '''A method of the class indended for printing out all of the annotation in the ConLLu format.\n",
    "        '''\n",
    "        for sentence in self.all_conll_sents:\n",
    "            print(sentence.return_sent())\n",
    "            print('\\n')\n",
    "        \n",
    "    def write_conllu_2_file(self, filename: str):\n",
    "        '''A method of the class indended for displaying saving all of the annotation in the ConLLu format.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): The name of the file the data should be saved to.\n",
    "        '''      \n",
    "        with open(filename, 'w') as f:\n",
    "            for sentence in self.all_conll_sents:\n",
    "                f.write(sentence.return_sent() + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6d9c5",
   "metadata": {},
   "source": [
    "### EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f90ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = OriginalAnnotations(file, lowercase=False)\n",
    "#print(original_text.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf735c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter = ConlluFormatter(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b11557",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter.print_conllu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatter.write_conllu_2_file('../data/memoirs_10k.conllu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
